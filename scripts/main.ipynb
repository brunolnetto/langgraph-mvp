{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac2003c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pydantic-graph in /home/suavendas/.local/lib/python3.12/site-packages (0.1.8)\n",
      "Requirement already satisfied: httpx>=0.27 in /home/suavendas/.local/lib/python3.12/site-packages (from pydantic-graph) (0.28.1)\n",
      "Requirement already satisfied: logfire-api>=1.2.0 in /home/suavendas/.local/lib/python3.12/site-packages (from pydantic-graph) (3.14.1)\n",
      "Requirement already satisfied: pydantic>=2.10 in /home/suavendas/.local/lib/python3.12/site-packages (from pydantic-graph) (2.11.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/suavendas/.local/lib/python3.12/site-packages (from pydantic-graph) (0.4.0)\n",
      "Requirement already satisfied: anyio in /home/suavendas/.local/lib/python3.12/site-packages (from httpx>=0.27->pydantic-graph) (4.9.0)\n",
      "Requirement already satisfied: certifi in /home/suavendas/.local/lib/python3.12/site-packages (from httpx>=0.27->pydantic-graph) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/suavendas/.local/lib/python3.12/site-packages (from httpx>=0.27->pydantic-graph) (1.0.7)\n",
      "Requirement already satisfied: idna in /usr/lib/python3/dist-packages (from httpx>=0.27->pydantic-graph) (3.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/suavendas/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27->pydantic-graph) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/suavendas/.local/lib/python3.12/site-packages (from pydantic>=2.10->pydantic-graph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/suavendas/.local/lib/python3.12/site-packages (from pydantic>=2.10->pydantic-graph) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/suavendas/.local/lib/python3.12/site-packages (from pydantic>=2.10->pydantic-graph) (4.13.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/suavendas/.local/lib/python3.12/site-packages (from anyio->httpx>=0.27->pydantic-graph) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --break-system-packages pydantic-graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ee49855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "👤 Usuário: Qual o preço do laptop?\n",
      "🤖 Desculpe, mas não consegui encontrar informações sobre o preço do laptop. Você poderia fornecer mais detalhes, como a marca ou o modelo específico que está procurando?\n",
      "\n",
      "👤 Usuário: E o mouse está disponível?\n",
      "🤖 Atualmente, não há informações disponíveis sobre a disponibilidade do mouse. Se precisar de mais detalhes ou se estiver buscando um modelo específico, por favor, me avise!\n",
      "\n",
      "👤 Usuário: Qual o status do pedido #XYZ123?\n",
      "🤖 O status do pedido #XYZ123 foi verificado com sucesso, mas não foram encontrados detalhes disponíveis sobre ele. Recomendo verificar novamente ou, se preferir, fornecer mais informações para que eu possa ajudar melhor.\n",
      "\n",
      "👤 Usuário: Quero reclamar do pedido #XYZ123, veio com defeito.\n",
      "🤖 Sua reclamação sobre o pedido #XYZ123 foi registrada com sucesso devido ao defeito identificado. Se precisar de mais alguma coisa, estou à disposição para ajudar!\n",
      "\n",
      "👤 Usuário: Qual o preço do hoverboard?\n",
      "🤖 Desculpe, não consegui encontrar informações sobre o preço do hoverboard. Você pode tentar novamente ou verificar outros produtos disponíveis. Se precisar de ajuda com algo específico, estou à disposição!\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from functools import lru_cache, wraps, partial\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import Union, Literal, Optional, Callable, Awaitable\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "# ===========================\n",
    "# 📦 MODELOS DE RESPOSTA\n",
    "# ===========================\n",
    "\n",
    "class ToolSuccess(BaseModel):\n",
    "    type: Literal[\"success\"] = \"success\"\n",
    "    tool_name: str\n",
    "    result: BaseModel  # O resultado real da tool (ProductInfo, OrderStatus, etc)\n",
    "\n",
    "class ToolFailure(BaseModel):\n",
    "    type: Literal[\"failure\"] = \"failure\"\n",
    "    tool_name: str\n",
    "    error_message: str\n",
    "    input_params: Optional[dict] = None\n",
    "\n",
    "ToolResult = Union[ToolSuccess, ToolFailure]\n",
    "\n",
    "class ProductInfo(BaseModel):\n",
    "    name: str\n",
    "    price: float\n",
    "    available: bool\n",
    "\n",
    "class OrderStatus(BaseModel):\n",
    "    order_id: str\n",
    "    status: Literal[\"processing\", \"shipped\", \"delivered\", \"cancelled\"]\n",
    "\n",
    "class ComplaintResponse(BaseModel):\n",
    "    complaint_id: str\n",
    "    status: str\n",
    "    resolution_eta: Optional[str] = None\n",
    "\n",
    "# ===========================\n",
    "# 🛠️ FUNÇÕES AUXILIARES\n",
    "# ==========================\n",
    "\n",
    "def concurrent_tool(tool_func):\n",
    "    @wraps(tool_func)\n",
    "    async def wrapper(*args, **kwargs):\n",
    "        return await asyncio.to_thread(tool_func, *args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "async def execute_tool_safely(\n",
    "    tool_func: Callable[..., Awaitable[BaseModel]],\n",
    "    tool_name: str,\n",
    "    **kwargs\n",
    ") -> ToolResult:\n",
    "    try:\n",
    "        result = await tool_func(**kwargs)\n",
    "        return ToolSuccess(tool_name=tool_name, result=result)\n",
    "    except Exception as e:\n",
    "        return ToolFailure(\n",
    "            tool_name=tool_name,\n",
    "            error_message=str(e),\n",
    "            input_params=kwargs\n",
    "        )\n",
    "\n",
    "# ===========================\n",
    "# 🛠️ TOOLS FUNCIONAIS\n",
    "# ===========================\n",
    "\n",
    "@lru_cache\n",
    "def get_fake_product_data():\n",
    "    return {\n",
    "        \"laptop\": {\"name\": \"Laptop Pro X\", \"price\": 1499.99, \"available\": True},\n",
    "        \"mouse\": {\"name\": \"Mouse Ergonômico\", \"price\": 49.90, \"available\": False},\n",
    "    }\n",
    "\n",
    "async def get_product_info(product_name: str) -> ProductInfo:\n",
    "    await asyncio.sleep(0.1)  # menos latência\n",
    "    fake_db = get_fake_product_data()\n",
    "    product = fake_db.get(product_name.lower())\n",
    "    if not product:\n",
    "        raise ValueError(f\"Produto '{product_name}' não encontrado.\")\n",
    "    return ProductInfo(**product)\n",
    "\n",
    "\n",
    "async def check_order_status(order_id: str) -> OrderStatus:\n",
    "    await asyncio.sleep(0.2)\n",
    "    return OrderStatus(order_id=order_id, status=\"shipped\")\n",
    "\n",
    "async def register_complaint(order_id: str, reason: str) -> ComplaintResponse:\n",
    "    await asyncio.sleep(0.5)\n",
    "    return ComplaintResponse(\n",
    "        complaint_id=f\"C-{order_id}\",\n",
    "        status=\"received\",\n",
    "        resolution_eta=\"3 dias úteis\"\n",
    "    )\n",
    "\n",
    "# ===========================\n",
    "# 🤖 AGENTE INTELIGENTE\n",
    "# ===========================\n",
    "\n",
    "async def safe_get_product_info(product_name: str) -> ToolResult:\n",
    "    result = await execute_tool_safely(get_product_info, tool_name=\"get_product_info\", product_name=product_name)\n",
    "    if isinstance(result, ToolFailure):\n",
    "        result.error_message = f\"Desculpe, não conseguimos encontrar o produto '{product_name}'. Por favor, tente novamente ou veja outros produtos disponíveis.\"\n",
    "    return result\n",
    "\n",
    "\n",
    "async def safe_check_order_status(order_id: str) -> ToolResult:\n",
    "    return await execute_tool_safely(check_order_status, tool_name=\"check_order_status\", order_id=order_id)\n",
    "\n",
    "async def safe_register_complaint(order_id: str, reason: str) -> ToolResult:\n",
    "    return await execute_tool_safely(register_complaint, tool_name=\"register_complaint\", order_id=order_id, reason=reason)\n",
    "\n",
    "tools=[\n",
    "    safe_get_product_info,\n",
    "    safe_check_order_status,\n",
    "    safe_register_complaint\n",
    "]\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "system_prompt = \"\"\"\n",
    "Você é um agente de atendimento ao cliente.\n",
    "Você deve responder perguntas de forma educada e objetiva, chamando ferramentas quando necessário.\n",
    "\"\"\"\n",
    "\n",
    "agent = Agent(model=model_name, system_prompt=system_prompt, tools=tools, memory=False)\n",
    "\n",
    "# ===========================\n",
    "# 🚀 EXECUÇÃO INTERATIVA\n",
    "# ===========================\n",
    "perguntas = [\n",
    "    \"Qual o preço do laptop?\",\n",
    "    \"E o mouse está disponível?\",\n",
    "    \"Qual o status do pedido #XYZ123?\",\n",
    "    \"Quero reclamar do pedido #XYZ123, veio com defeito.\",\n",
    "    \"Qual o preço do hoverboard?\"\n",
    "]\n",
    "\n",
    "\n",
    "responses = await asyncio.gather(*(agent.run(p) for p in perguntas))\n",
    "for pergunta, response in zip(perguntas, responses):\n",
    "    result = response.output\n",
    "    print(f\"\\n👤 Usuário: {pergunta}\")\n",
    "    if isinstance(result, ToolSuccess):\n",
    "        print(f\"✅ [{result.tool_name}] → {result.result}\")\n",
    "    elif isinstance(result, ToolFailure):\n",
    "        print(f\"❌ [{result.tool_name}] Falha: {result.error_message}\")\n",
    "    else:\n",
    "        print(f\"🤖 {result}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "abdb121a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Select product: </pre>\n"
      ],
      "text/plain": [
       "Select product: "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purchase successful item=water change=8.75\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from rich.prompt import Prompt\n",
    "\n",
    "from pydantic_graph import BaseNode, End, Graph, GraphRunContext\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MachineState:  \n",
    "    user_balance: float = 0.0\n",
    "    product: str | None = None\n",
    "\n",
    "coins_inserted = 10\n",
    "\n",
    "@dataclass\n",
    "class InsertCoin(BaseNode[MachineState]):  \n",
    "    async def run(self, ctx: GraphRunContext[MachineState]) -> CoinsInserted:  \n",
    "        return CoinsInserted(coins_inserted)  \n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CoinsInserted(BaseNode[MachineState]):\n",
    "    amount: float  \n",
    "\n",
    "    async def run(\n",
    "        self, ctx: GraphRunContext[MachineState]\n",
    "    ) -> SelectProduct | Purchase:  \n",
    "        ctx.state.user_balance += self.amount  \n",
    "        if ctx.state.product is not None:  \n",
    "            return Purchase(ctx.state.product)\n",
    "        else:\n",
    "            return SelectProduct()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SelectProduct(BaseNode[MachineState]):\n",
    "    async def run(self, ctx: GraphRunContext[MachineState]) -> Purchase:\n",
    "        return Purchase(Prompt.ask('Select product'))\n",
    "\n",
    "\n",
    "PRODUCT_PRICES = {  \n",
    "    'water': 1.25,\n",
    "    'soda': 1.50,\n",
    "    'crisps': 1.75,\n",
    "    'chocolate': 2.00,\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Purchase(BaseNode[MachineState, None, None]):  \n",
    "    product: str\n",
    "\n",
    "    async def run(\n",
    "        self, ctx: GraphRunContext[MachineState]\n",
    "    ) -> End | InsertCoin | SelectProduct:\n",
    "        if price := PRODUCT_PRICES.get(self.product):  \n",
    "            ctx.state.product = self.product  \n",
    "            if ctx.state.user_balance >= price:  \n",
    "                ctx.state.user_balance -= price\n",
    "                return End(None)\n",
    "            else:\n",
    "                diff = price - ctx.state.user_balance\n",
    "                print(f'Not enough money for {self.product}, need {diff:0.2f} more')\n",
    "                #> Not enough money for crisps, need 0.75 more\n",
    "                return InsertCoin()  \n",
    "        else:\n",
    "            print(f'No such product: {self.product}, try again')\n",
    "            return SelectProduct()  \n",
    "\n",
    "\n",
    "vending_machine_graph = Graph(  \n",
    "    nodes=[InsertCoin, CoinsInserted, SelectProduct, Purchase]\n",
    ")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    state = MachineState()  \n",
    "    await vending_machine_graph.run(InsertCoin(), state=state)  \n",
    "    print(f'purchase successful item={state.product} change={state.user_balance:0.2f}')\n",
    "    #> purchase successful item=crisps change=0.25\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc615ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: openai:gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROVIDER_NAME = os.getenv('PYDANTIC_AI_PROVIDER', 'openai')\n",
    "MODEL_NAME = os.getenv('PYDANTIC_AI_MODEL', 'gpt-4o-mini')\n",
    "pydantic_model = f'{PROVIDER_NAME}:{MODEL_NAME}'\n",
    "print(f'Using model: {pydantic_model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aadab712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:26:22.826 agent run\n",
      "13:26:22.830   chat gpt-4o-mini\n",
      "city='Chicago' country='United States'\n",
      "Usage(requests=1, request_tokens=58, response_tokens=20, total_tokens=78, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0, 'cached_tokens': 0})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import logfire\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "\n",
    "class MyModel(BaseModel):\n",
    "    city: str\n",
    "    country: str\n",
    "\n",
    "agent = Agent(pydantic_model, output_type=MyModel, instrument=True)\n",
    "result = await agent.run('The windy city in the US of A.')\n",
    "\n",
    "print(result.output)\n",
    "print(result.usage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:26:44.611 weather_agent run\n",
      "13:26:44.613   chat gpt-4o-mini\n",
      "13:26:46.357   running 2 tools\n",
      "13:26:46.357     running tool: get_lat_lng\n",
      "13:26:46.357     running tool: get_lat_lng\n",
      "13:26:46.358   chat gpt-4o-mini\n",
      "13:26:48.017   running 2 tools\n",
      "13:26:48.017     running tool: get_weather\n",
      "13:26:48.018     running tool: get_weather\n",
      "13:26:48.019   chat gpt-4o-mini\n",
      "/tmp/ipykernel_6624/3760346197.py:141 main\n",
      "    result: AgentRunResult(\n",
      "        output='The weather in both London and Wiltshire is sunny with a temperature of 21 °C.',\n",
      "        _output_tool_name=None,\n",
      "        _state=GraphAgentState(\n",
      "            message_history=[\n",
      "                ModelRequest(\n",
      "                    parts=[\n",
      "                        SystemPromptPart(\n",
      "                            content=(\n",
      "                                'Be concise, reply with one sentence.Use the `get_lat_lng` tool to get the latitude an'\n",
      "                                'd longitude of the locations, then use the `get_weather` tool to get the weather.'\n",
      "                            ),\n",
      "                            timestamp=datetime.datetime(2025, 5, 7, 13, 26, 44, 612566, tzinfo=datetime.timezone.utc),\n",
      "                            dynamic_ref=None,\n",
      "                            part_kind='system-prompt',\n",
      "                        ),\n",
      "                        UserPromptPart(\n",
      "                            content='What is the weather like in London and in Wiltshire?',\n",
      "                            timestamp=datetime.datetime(2025, 5, 7, 13, 26, 44, 612576, tzinfo=datetime.timezone.utc),\n",
      "                            part_kind='user-prompt',\n",
      "                        ),\n",
      "                    ],\n",
      "                    instructions=None,\n",
      "                    kind='request',\n",
      "                ),\n",
      "                ModelResponse(\n",
      "                    parts=[\n",
      "                        ToolCallPart(\n",
      "                            tool_name='get_lat_lng',\n",
      "                            args='{\"location_description\": \"London, UK\"}',\n",
      "                            tool_call_id='call_rkAef9Xa06zBD5kZKxm7Qp8m',\n",
      "                            part_kind='tool-call',\n",
      "                        ),\n",
      "                        ToolCallPart(\n",
      "                            tool_name='get_lat_lng',\n",
      "                            args='{\"location_description\": \"Wiltshire, UK\"}',\n",
      "                            tool_call_id='call_mWeZGPean0G3TyZrzzi762Ru',\n",
      "                            part_kind='tool-call',\n",
      "                        ),\n",
      "                    ],\n",
      "                    model_name='gpt-4o-mini-2024-07-18',\n",
      "                    timestamp=datetime.datetime(2025, 5, 7, 13, 26, 45, tzinfo=datetime.timezone.utc),\n",
      "                    kind='response',\n",
      "                ),\n",
      "                ModelRequest(\n",
      "                    parts=[\n",
      "                        ToolReturnPart(\n",
      "                            tool_name='get_lat_lng',\n",
      "                            content={\n",
      "                                'lat': 51.1,\n",
      "                                'lng': -0.1,\n",
      "                            },\n",
      "                            tool_call_id='call_rkAef9Xa06zBD5kZKxm7Qp8m',\n",
      "                            timestamp=datetime.datetime(2025, 5, 7, 13, 26, 46, 357804, tzinfo=datetime.timezone.utc),\n",
      "                            part_kind='tool-return',\n",
      "                        ),\n",
      "                        ToolReturnPart(\n",
      "                            tool_name='get_lat_lng',\n",
      "                            content={\n",
      "                                'lat': 51.1,\n",
      "                                'lng': -0.1,\n",
      "                            },\n",
      "                            tool_call_id='call_mWeZGPean0G3TyZrzzi762Ru',\n",
      "                            timestamp=datetime.datetime(2025, 5, 7, 13, 26, 46, 357982, tzinfo=datetime.timezone.utc),\n",
      "                            part_kind='tool-return',\n",
      "                        ),\n",
      "                    ],\n",
      "                    instructions=None,\n",
      "                    kind='request',\n",
      "                ),\n",
      "                ModelResponse(\n",
      "                    parts=[\n",
      "                        ToolCallPart(\n",
      "                            tool_name='get_weather',\n",
      "                            args='{\"lat\": 51.1, \"lng\": -0.1}',\n",
      "                            tool_call_id='call_gqSqJnvycm5z0hRmFnmUU6DG',\n",
      "                            part_kind='tool-call',\n",
      "                        ),\n",
      "                        ToolCallPart(\n",
      "                            tool_name='get_weather',\n",
      "                            args='{\"lat\": 51.1, \"lng\": -0.1}',\n",
      "                            tool_call_id='call_0BTlQ3bQFioIyFcaW9ajkNvg',\n",
      "                            part_kind='tool-call',\n",
      "                        ),\n",
      "                    ],\n",
      "                    model_name='gpt-4o-mini-2024-07-18',\n",
      "                    timestamp=datetime.datetime(2025, 5, 7, 13, 26, 46, tzinfo=datetime.timezone.utc),\n",
      "                    kind='response',\n",
      "                ),\n",
      "                ModelRequest(\n",
      "                    parts=[\n",
      "                        ToolReturnPart(\n",
      "                            tool_name='get_weather',\n",
      "                            content={\n",
      "                                'temperature': '21 °C',\n",
      "                                'description': 'Sunny',\n",
      "                            },\n",
      "                            tool_call_id='call_gqSqJnvycm5z0hRmFnmUU6DG',\n",
      "                            timestamp=datetime.datetime(2025, 5, 7, 13, 26, 48, 18076, tzinfo=datetime.timezone.utc),\n",
      "                            part_kind='tool-return',\n",
      "                        ),\n",
      "                        ToolReturnPart(\n",
      "                            tool_name='get_weather',\n",
      "                            content={\n",
      "                                'temperature': '21 °C',\n",
      "                                'description': 'Sunny',\n",
      "                            },\n",
      "                            tool_call_id='call_0BTlQ3bQFioIyFcaW9ajkNvg',\n",
      "                            timestamp=datetime.datetime(2025, 5, 7, 13, 26, 48, 18419, tzinfo=datetime.timezone.utc),\n",
      "                            part_kind='tool-return',\n",
      "                        ),\n",
      "                    ],\n",
      "                    instructions=None,\n",
      "                    kind='request',\n",
      "                ),\n",
      "                ModelResponse(\n",
      "                    parts=[\n",
      "                        TextPart(\n",
      "                            content='The weather in both London and Wiltshire is sunny with a temperature of 21 °C.',\n",
      "                            part_kind='text',\n",
      "                        ),\n",
      "                    ],\n",
      "                    model_name='gpt-4o-mini-2024-07-18',\n",
      "                    timestamp=datetime.datetime(2025, 5, 7, 13, 26, 48, tzinfo=datetime.timezone.utc),\n",
      "                    kind='response',\n",
      "                ),\n",
      "            ],\n",
      "            usage=Usage(\n",
      "                requests=3,\n",
      "                request_tokens=735,\n",
      "                response_tokens=140,\n",
      "                total_tokens=875,\n",
      "                details={\n",
      "                    'accepted_prediction_tokens': 0,\n",
      "                    'audio_tokens': 0,\n",
      "                    'reasoning_tokens': 0,\n",
      "                    'rejected_prediction_tokens': 0,\n",
      "                    'cached_tokens': 0,\n",
      "                },\n",
      "            ),\n",
      "            retries=0,\n",
      "            run_step=3,\n",
      "        ),\n",
      "        _new_message_index=0,\n",
      "        _traceparent_value='00-0196aaed5c837e96c768e20c9df13236-08f294df8fb31bd9-01',\n",
      "    ) (AgentRunResult)\n",
      "Response: The weather in both London and Wiltshire is sunny with a temperature of 21 °C.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations as _annotations\n",
    "\n",
    "import asyncio\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "\n",
    "import logfire\n",
    "from devtools import debug\n",
    "from httpx import AsyncClient\n",
    "\n",
    "from pydantic_ai import Agent, ModelRetry, RunContext\n",
    "\n",
    "# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\n",
    "logfire.configure(send_to_logfire='if-token-present')\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Deps:\n",
    "    client: AsyncClient\n",
    "    weather_api_key: str | None\n",
    "    geo_api_key: str | None\n",
    "\n",
    "\n",
    "weather_agent = Agent(\n",
    "    pydantic_model,\n",
    "    # 'Be concise, reply with one sentence.' is enough for some models (like openai) to use\n",
    "    # the below tools appropriately, but others like anthropic and gemini require a bit more direction.\n",
    "    system_prompt=(\n",
    "        'Be concise, reply with one sentence.'\n",
    "        'Use the `get_lat_lng` tool to get the latitude and longitude of the locations, '\n",
    "        'then use the `get_weather` tool to get the weather.'\n",
    "    ),\n",
    "    deps_type=Deps,\n",
    "    retries=2,\n",
    "    instrument=True,\n",
    ")\n",
    "\n",
    "\n",
    "@weather_agent.tool\n",
    "async def get_lat_lng(\n",
    "    ctx: RunContext[Deps], location_description: str\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"Get the latitude and longitude of a location.\n",
    "\n",
    "    Args:\n",
    "        ctx: The context.\n",
    "        location_description: A description of a location.\n",
    "    \"\"\"\n",
    "    if ctx.deps.geo_api_key is None:\n",
    "        # if no API key is provided, return a dummy response (London)\n",
    "        return {'lat': 51.1, 'lng': -0.1}\n",
    "\n",
    "    params = {\n",
    "        'q': location_description,\n",
    "        'api_key': ctx.deps.geo_api_key,\n",
    "    }\n",
    "    with logfire.span('calling geocode API', params=params) as span:\n",
    "        r = await ctx.deps.client.get('https://geocode.maps.co/search', params=params)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        span.set_attribute('response', data)\n",
    "\n",
    "    if data:\n",
    "        return {'lat': data[0]['lat'], 'lng': data[0]['lon']}\n",
    "    else:\n",
    "        raise ModelRetry('Could not find the location')\n",
    "\n",
    "\n",
    "@weather_agent.tool\n",
    "async def get_weather(ctx: RunContext[Deps], lat: float, lng: float) -> dict[str, Any]:\n",
    "    \"\"\"Get the weather at a location.\n",
    "\n",
    "    Args:\n",
    "        ctx: The context.\n",
    "        lat: Latitude of the location.\n",
    "        lng: Longitude of the location.\n",
    "    \"\"\"\n",
    "    if ctx.deps.weather_api_key is None:\n",
    "        # if no API key is provided, return a dummy response\n",
    "        return {'temperature': '21 °C', 'description': 'Sunny'}\n",
    "\n",
    "    params = {\n",
    "        'apikey': ctx.deps.weather_api_key,\n",
    "        'location': f'{lat},{lng}',\n",
    "        'units': 'metric',\n",
    "    }\n",
    "    with logfire.span('calling weather API', params=params) as span:\n",
    "        r = await ctx.deps.client.get(\n",
    "            'https://api.tomorrow.io/v4/weather/realtime', params=params\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        span.set_attribute('response', data)\n",
    "\n",
    "    values = data['data']['values']\n",
    "    # https://docs.tomorrow.io/reference/data-layers-weather-codes\n",
    "    code_lookup = {\n",
    "        1000: 'Clear, Sunny',\n",
    "        1100: 'Mostly Clear',\n",
    "        1101: 'Partly Cloudy',\n",
    "        1102: 'Mostly Cloudy',\n",
    "        1001: 'Cloudy',\n",
    "        2000: 'Fog',\n",
    "        2100: 'Light Fog',\n",
    "        4000: 'Drizzle',\n",
    "        4001: 'Rain',\n",
    "        4200: 'Light Rain',\n",
    "        4201: 'Heavy Rain',\n",
    "        5000: 'Snow',\n",
    "        5001: 'Flurries',\n",
    "        5100: 'Light Snow',\n",
    "        5101: 'Heavy Snow',\n",
    "        6000: 'Freezing Drizzle',\n",
    "        6001: 'Freezing Rain',\n",
    "        6200: 'Light Freezing Rain',\n",
    "        6201: 'Heavy Freezing Rain',\n",
    "        7000: 'Ice Pellets',\n",
    "        7101: 'Heavy Ice Pellets',\n",
    "        7102: 'Light Ice Pellets',\n",
    "        8000: 'Thunderstorm',\n",
    "    }\n",
    "    return {\n",
    "        'temperature': f'{values[\"temperatureApparent\"]:0.0f}°C',\n",
    "        'description': code_lookup.get(values['weatherCode'], 'Unknown'),\n",
    "    }\n",
    "\n",
    "\n",
    "async def main():\n",
    "    async with AsyncClient() as client:\n",
    "        # create a free API key at https://www.tomorrow.io/weather-api/\n",
    "        weather_api_key = os.getenv('WEATHER_API_KEY')\n",
    "        # create a free API key at https://geocode.maps.co/\n",
    "        geo_api_key = os.getenv('GEO_API_KEY')\n",
    "        deps = Deps(\n",
    "            client=client, weather_api_key=weather_api_key, geo_api_key=geo_api_key\n",
    "        )\n",
    "        result = await weather_agent.run(\n",
    "            'What is the weather like in London and in Wiltshire?', deps=deps\n",
    "        )\n",
    "        debug(result)\n",
    "        print('Response:', result.output)\n",
    "\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ae81794a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n"
     ]
    }
   ],
   "source": [
    "!pip install 'logfire[asyncpg]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fa8a4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to instrument while already instrumented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:29:31.411 check and create DB\n",
      "13:29:31.461   SELECT\n",
      "13:29:31.506 create schema\n",
      "13:29:31.507   BEGIN;\n",
      "13:29:31.508   COMMIT;\n",
      "13:29:31.510 agent run\n",
      "13:29:31.511   chat gpt-4o-mini\n",
      "/tmp/ipykernel_6624/2723136973.py:164 main\n",
      "    result.output: InvalidRequest(\n",
      "        error_message=\"Your request doesn't contain enough information to generate a SQL query.\",\n",
      "    ) (InvalidRequest)\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import sys\n",
    "from collections.abc import AsyncGenerator\n",
    "from contextlib import asynccontextmanager\n",
    "from dataclasses import dataclass\n",
    "from datetime import date\n",
    "from typing import Annotated, Any, Union\n",
    "\n",
    "import asyncpg\n",
    "import logfire\n",
    "from annotated_types import MinLen\n",
    "from devtools import debug\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import TypeAlias\n",
    "\n",
    "from pydantic_ai import Agent, ModelRetry, RunContext, format_as_xml\n",
    "\n",
    "# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\n",
    "logfire.configure(send_to_logfire='if-token-present')\n",
    "logfire.instrument_asyncpg()\n",
    "\n",
    "DB_SCHEMA = \"\"\"\n",
    "CREATE TABLE records (\n",
    "    created_at timestamptz,\n",
    "    start_timestamp timestamptz,\n",
    "    end_timestamp timestamptz,\n",
    "    trace_id text,\n",
    "    span_id text,\n",
    "    parent_span_id text,\n",
    "    level log_level,\n",
    "    span_name text,\n",
    "    message text,\n",
    "    attributes_json_schema text,\n",
    "    attributes jsonb,\n",
    "    tags text[],\n",
    "    is_exception boolean,\n",
    "    otel_status_message text,\n",
    "    service_name text\n",
    ");\n",
    "\"\"\"\n",
    "SQL_EXAMPLES = [\n",
    "    {\n",
    "        'request': 'show me records where foobar is false',\n",
    "        'response': \"SELECT * FROM records WHERE attributes->>'foobar' = false\",\n",
    "    },\n",
    "    {\n",
    "        'request': 'show me records where attributes include the key \"foobar\"',\n",
    "        'response': \"SELECT * FROM records WHERE attributes ? 'foobar'\",\n",
    "    },\n",
    "    {\n",
    "        'request': 'show me records from yesterday',\n",
    "        'response': \"SELECT * FROM records WHERE start_timestamp::date > CURRENT_TIMESTAMP - INTERVAL '1 day'\",\n",
    "    },\n",
    "    {\n",
    "        'request': 'show me error records with the tag \"foobar\"',\n",
    "        'response': \"SELECT * FROM records WHERE level = 'error' and 'foobar' = ANY(tags)\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Deps:\n",
    "    conn: asyncpg.Connection\n",
    "\n",
    "\n",
    "class Success(BaseModel):\n",
    "    \"\"\"Response when SQL could be successfully generated.\"\"\"\n",
    "\n",
    "    sql_query: Annotated[str, MinLen(1)]\n",
    "    explanation: str = Field(\n",
    "        '', description='Explanation of the SQL query, as markdown'\n",
    "    )\n",
    "\n",
    "\n",
    "class InvalidRequest(BaseModel):\n",
    "    \"\"\"Response the user input didn't include enough information to generate SQL.\"\"\"\n",
    "\n",
    "    error_message: str\n",
    "\n",
    "\n",
    "Response: TypeAlias = Union[Success, InvalidRequest]\n",
    "agent: Agent[Deps, Response] = Agent(\n",
    "    pydantic_model,\n",
    "    # Type ignore while we wait for PEP-0747, nonetheless unions will work fine everywhere else\n",
    "    output_type=Response,  # type: ignore\n",
    "    deps_type=Deps,\n",
    "    instrument=True,\n",
    ")\n",
    "\n",
    "\n",
    "@agent.system_prompt\n",
    "async def system_prompt() -> str:\n",
    "    return f\"\"\"\\\n",
    "Given the following PostgreSQL table of records, your job is to\n",
    "write a SQL query that suits the user's request.\n",
    "\n",
    "Database schema:\n",
    "\n",
    "{DB_SCHEMA}\n",
    "\n",
    "today's date = {date.today()}\n",
    "\n",
    "{format_as_xml(SQL_EXAMPLES)}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@agent.output_validator\n",
    "async def validate_output(ctx: RunContext[Deps], output: Response) -> Response:\n",
    "    if isinstance(output, InvalidRequest):\n",
    "        return output\n",
    "\n",
    "    # gemini often adds extraneous backslashes to SQL\n",
    "    output.sql_query = output.sql_query.replace('\\\\', '')\n",
    "    if not output.sql_query.upper().startswith('SELECT'):\n",
    "        raise ModelRetry('Please create a SELECT query')\n",
    "\n",
    "    try:\n",
    "        await ctx.deps.conn.execute(f'EXPLAIN {output.sql_query}')\n",
    "    except asyncpg.exceptions.PostgresError as e:\n",
    "        raise ModelRetry(f'Invalid query: {e}') from e\n",
    "    else:\n",
    "        return output\n",
    "\n",
    "\n",
    "# pyright: reportUnknownMemberType=false\n",
    "# pyright: reportUnknownVariableType=false\n",
    "@asynccontextmanager\n",
    "async def database_connect(server_dsn: str, database: str) -> AsyncGenerator[Any, None]:\n",
    "    with logfire.span('check and create DB'):\n",
    "        conn = await asyncpg.connect(server_dsn)\n",
    "        try:\n",
    "            db_exists = await conn.fetchval(\n",
    "                'SELECT 1 FROM pg_database WHERE datname = $1', database\n",
    "            )\n",
    "            if not db_exists:\n",
    "                await conn.execute(f'CREATE DATABASE {database}')\n",
    "        finally:\n",
    "            await conn.close()\n",
    "\n",
    "    conn = await asyncpg.connect(f'{server_dsn}/{database}')\n",
    "    try:\n",
    "        with logfire.span('create schema'):\n",
    "            async with conn.transaction():\n",
    "                if not db_exists:\n",
    "                    await conn.execute(\n",
    "                        \"CREATE TYPE log_level AS ENUM ('debug', 'info', 'warning', 'error', 'critical')\"\n",
    "                    )\n",
    "                    await conn.execute(DB_SCHEMA)\n",
    "        yield conn\n",
    "    finally:\n",
    "        await conn.close()\n",
    "\n",
    "async def main():\n",
    "    if len(sys.argv) == 1:\n",
    "        prompt = 'show me logs from yesterday, with level \"error\"'\n",
    "    else:\n",
    "        prompt = sys.argv[1]\n",
    "\n",
    "    async with database_connect(\n",
    "        'postgresql://postgres:postgres@localhost:5432', 'pydantic_ai_sql_gen'\n",
    "    ) as conn:\n",
    "        deps = Deps(conn)\n",
    "        result = await agent.run(prompt, deps=deps)\n",
    "    debug(result.output)\n",
    "\n",
    "\n",
    "\n",
    "await main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7eb2baa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/suavendas/.local/lib/python3.12/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/suavendas/.local/lib/python3.12/site-packages (from ipywidgets) (9.1.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/suavendas/.local/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: decorator in /home/suavendas/.local/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /home/suavendas/.local/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/suavendas/.local/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/suavendas/.local/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/suavendas/.local/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/suavendas/.local/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/suavendas/.local/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /home/suavendas/.local/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/suavendas/.local/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/suavendas/.local/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/suavendas/.local/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/suavendas/.local/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/suavendas/.local/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /home/suavendas/.local/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/suavendas/.local/lib/python3.12/site-packages (from asttokens>=2.1.0->stack_data->ipython>=6.1.0->ipywidgets) (1.17.0)\n",
      "Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 widgetsnbextension-4.0.14\n"
     ]
    }
   ],
   "source": [
    "!pip install --break-system-package ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c9ece1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[10:39:26] </span><span style=\"color: #008080; text-decoration-color: #008080\">Asking: Show me a short example of using Pydantic</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"color: #008080; text-decoration-color: #008080\">.                                    </span> <a href=\"file:///tmp/ipykernel_6624/68239946.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">68239946.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_6624/68239946.py#30\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">30</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[10:39:26]\u001b[0m\u001b[2;36m \u001b[0m\u001b[36mAsking: Show me a short example of using Pydantic\u001b[0m\u001b[33m...\u001b[0m\u001b[36m.                                    \u001b[0m \u001b]8;id=187803;file:///tmp/ipykernel_6624/68239946.py\u001b\\\u001b[2m68239946.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=837985;file:///tmp/ipykernel_6624/68239946.py#30\u001b\\\u001b[2m30\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>Using model: openai:gpt-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.5</span>-turbo                                                         <a href=\"file:///tmp/ipykernel_6624/68239946.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">68239946.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_6624/68239946.py#33\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">33</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mUsing model: openai:gpt-\u001b[1;36m3.5\u001b[0m-turbo                                                         \u001b]8;id=810442;file:///tmp/ipykernel_6624/68239946.py\u001b\\\u001b[2m68239946.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=698811;file:///tmp/ipykernel_6624/68239946.py#33\u001b\\\u001b[2m33\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a5b1e3d38c44d2087c833591d638bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "\n",
    "import logfire\n",
    "from rich.console import Console, ConsoleOptions, RenderResult\n",
    "from rich.live import Live\n",
    "from rich.markdown import CodeBlock, Markdown\n",
    "from rich.syntax import Syntax\n",
    "from rich.text import Text\n",
    "\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models import KnownModelName\n",
    "\n",
    "# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\n",
    "logfire.configure(send_to_logfire='if-token-present')\n",
    "\n",
    "agent = Agent(instrument=True)\n",
    "\n",
    "# models to try, and the appropriate env var\n",
    "models: list[tuple[KnownModelName, str]] = [\n",
    "    ('openai:gpt-3.5-turbo', 'OPENAI_API_KEY'),\n",
    "    ('openai:gpt-4o-mini', 'OPENAI_API_KEY'),\n",
    "]\n",
    "\n",
    "\n",
    "async def main():\n",
    "    prettier_code_blocks()\n",
    "    console = Console()\n",
    "    prompt = 'Show me a short example of using Pydantic.'\n",
    "    console.log(f'Asking: {prompt}...', style='cyan')\n",
    "    for model, env_var in models:\n",
    "        if env_var in os.environ:\n",
    "            console.log(f'Using model: {model}')\n",
    "            with Live('', console=console, vertical_overflow='visible') as live:\n",
    "                async with agent.run_stream(prompt, model=model) as result:\n",
    "                    async for message in result.stream():\n",
    "                        live.update(Markdown(message))\n",
    "            console.log(result.usage())\n",
    "        else:\n",
    "            console.log(f'{model} requires {env_var} to be set.')\n",
    "\n",
    "\n",
    "def prettier_code_blocks():\n",
    "    \"\"\"Make rich code blocks prettier and easier to copy.\n",
    "\n",
    "    From https://github.com/samuelcolvin/aicli/blob/v0.8.0/samuelcolvin_aicli.py#L22\n",
    "    \"\"\"\n",
    "\n",
    "    class SimpleCodeBlock(CodeBlock):\n",
    "        def __rich_console__(\n",
    "            self, console: Console, options: ConsoleOptions\n",
    "        ) -> RenderResult:\n",
    "            code = str(self.text).rstrip()\n",
    "            yield Text(self.lexer_name, style='dim')\n",
    "            yield Syntax(\n",
    "                code,\n",
    "                self.lexer_name,\n",
    "                theme=self.theme,\n",
    "                background_color='default',\n",
    "                word_wrap=True,\n",
    "            )\n",
    "            yield Text(f'/{self.lexer_name}', style='dim')\n",
    "\n",
    "    Markdown.elements['fence'] = SimpleCodeBlock\n",
    "\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "202f7e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:48:14.584 run graph question_graph\n",
      "13:48:14.584   run node Ask\n",
      "13:48:14.586     ask_agent run\n",
      "13:48:14.588       chat gpt-4o\n",
      "13:48:15.176   run node Answer\n",
      "13:48:22.329   run node Evaluate\n",
      "13:48:23.585   run node Reprimand\n",
      "Comment: The answer is incorrect. The capital city of France is Paris.\n",
      "13:48:23.585   run node Ask\n",
      "13:48:23.586     ask_agent run\n",
      "13:48:23.586       chat gpt-4o\n",
      "13:48:24.488   run node Answer\n",
      "13:48:30.628   run node Evaluate\n",
      "13:48:31.505   run node Reprimand\n",
      "Comment: The answer is incorrect. The chemical symbol for water is H2O.\n",
      "13:48:31.506   run node Ask\n",
      "13:48:31.506     ask_agent run\n",
      "13:48:31.507       chat gpt-4o\n",
      "13:48:32.167   run node Answer\n",
      "13:48:43.315   run node Evaluate\n",
      "13:48:44.606   run node Reprimand\n",
      "Comment: The answer is incorrect. The largest planet in our solar system is Jupiter.\n",
      "13:48:44.607   run node Ask\n",
      "13:48:44.607     ask_agent run\n",
      "13:48:44.608       chat gpt-4o\n",
      "13:48:45.544   run node Answer\n",
      "13:48:54.060   run node Evaluate\n",
      "END: The answer is correct. The longest river in the world is the Amazon River.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations as _annotations\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "import logfire\n",
    "from groq import BaseModel\n",
    "from pydantic_graph import (\n",
    "    BaseNode,\n",
    "    End,\n",
    "    Graph,\n",
    "    GraphRunContext,\n",
    ")\n",
    "from pydantic_graph.persistence.file import FileStatePersistence\n",
    "\n",
    "from pydantic_ai import Agent, format_as_xml\n",
    "from pydantic_ai.messages import ModelMessage\n",
    "\n",
    "# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\n",
    "logfire.configure(send_to_logfire='if-token-present')\n",
    "\n",
    "ask_agent = Agent('openai:gpt-4o', output_type=str, instrument=True)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QuestionState:\n",
    "    question: str | None = None\n",
    "    ask_agent_messages: list[ModelMessage] = field(default_factory=list)\n",
    "    evaluate_agent_messages: list[ModelMessage] = field(default_factory=list)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Ask(BaseNode[QuestionState]):\n",
    "    async def run(self, ctx: GraphRunContext[QuestionState]) -> Answer:\n",
    "        result = await ask_agent.run(\n",
    "            'Ask a simple question with a single correct answer.',\n",
    "            message_history=ctx.state.ask_agent_messages,\n",
    "        )\n",
    "        ctx.state.ask_agent_messages += result.all_messages()\n",
    "        ctx.state.question = result.output\n",
    "        return Answer(result.output)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Answer(BaseNode[QuestionState]):\n",
    "    question: str\n",
    "\n",
    "    async def run(self, ctx: GraphRunContext[QuestionState]) -> Evaluate:\n",
    "        answer = input(f'{self.question}: ')\n",
    "        return Evaluate(answer)\n",
    "\n",
    "\n",
    "class EvaluationOutput(BaseModel, use_attribute_docstrings=True):\n",
    "    correct: bool\n",
    "    \"\"\"Whether the answer is correct.\"\"\"\n",
    "    comment: str\n",
    "    \"\"\"Comment on the answer, reprimand the user if the answer is wrong.\"\"\"\n",
    "\n",
    "\n",
    "evaluate_agent = Agent(\n",
    "    'openai:gpt-4o',\n",
    "    output_type=EvaluationOutput,\n",
    "    system_prompt='Given a question and answer, evaluate if the answer is correct.',\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Evaluate(BaseNode[QuestionState, None, str]):\n",
    "    answer: str\n",
    "\n",
    "    async def run(\n",
    "        self,\n",
    "        ctx: GraphRunContext[QuestionState],\n",
    "    ) -> End[str] | Reprimand:\n",
    "        assert ctx.state.question is not None\n",
    "        result = await evaluate_agent.run(\n",
    "            format_as_xml({'question': ctx.state.question, 'answer': self.answer}),\n",
    "            message_history=ctx.state.evaluate_agent_messages,\n",
    "        )\n",
    "        ctx.state.evaluate_agent_messages += result.all_messages()\n",
    "        if result.output.correct:\n",
    "            return End(result.output.comment)\n",
    "        else:\n",
    "            return Reprimand(result.output.comment)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Reprimand(BaseNode[QuestionState]):\n",
    "    comment: str\n",
    "\n",
    "    async def run(self, ctx: GraphRunContext[QuestionState]) -> Ask:\n",
    "        print(f'Comment: {self.comment}')\n",
    "        ctx.state.question = None\n",
    "        return Ask()\n",
    "\n",
    "\n",
    "question_graph = Graph(\n",
    "    nodes=(Ask, Answer, Evaluate, Reprimand), state_type=QuestionState\n",
    ")\n",
    "\n",
    "\n",
    "async def run_as_continuous():\n",
    "    state = QuestionState()\n",
    "    node = Ask()\n",
    "    end = await question_graph.run(node, state=state)\n",
    "    print('END:', end.output)\n",
    "\n",
    "\n",
    "async def run_as_cli(answer: str | None):\n",
    "    persistence = FileStatePersistence(Path('question_graph.json'))\n",
    "    persistence.set_graph_types(question_graph)\n",
    "\n",
    "    if snapshot := await persistence.load_next():\n",
    "        state = snapshot.state\n",
    "        assert answer is not None, (\n",
    "            'answer required, usage \"uv run -m pydantic_ai_examples.question_graph cli <answer>\"'\n",
    "        )\n",
    "        node = Evaluate(answer)\n",
    "    else:\n",
    "        state = QuestionState()\n",
    "        node = Ask()\n",
    "    # debug(state, node)\n",
    "\n",
    "    async with question_graph.iter(node, state=state, persistence=persistence) as run:\n",
    "        while True:\n",
    "            node = await run.next()\n",
    "            if isinstance(node, End):\n",
    "                print('END:', node.data)\n",
    "                history = await persistence.load_all()\n",
    "                print('history:', '\\n'.join(str(e.node) for e in history), sep='\\n')\n",
    "                print('Finished!')\n",
    "                break\n",
    "            elif isinstance(node, Answer):\n",
    "                print(node.question)\n",
    "                break\n",
    "            # otherwise just continue\n",
    "\n",
    "await run_as_continuous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4f87cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to instrument while already instrumented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:34:14.921 Asking \"How do I configure logfire to work with FastAPI?\"\n",
      "13:34:14.923 check and create DB\n",
      "13:34:14.953   SELECT\n",
      "13:34:14.971   CREATE\n",
      "13:34:15.360 agent run\n",
      "13:34:15.362   chat gpt-4o-mini\n",
      "13:34:16.658   running 1 tool\n",
      "13:34:16.658     running tool: retrieve\n",
      "13:34:16.660       create embedding for search_query=configure logfire with FastAPI\n",
      "13:34:16.666         Embedding Creation with 'text-embedding-3-small' [LLM]\n",
      "13:34:19.313       SELECT\n",
      "13:34:19.320       SELECT\n"
     ]
    },
    {
     "ename": "UndefinedTableError",
     "evalue": "relation \"doc_sections\" does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUndefinedTableError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 230\u001b[39m\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m action == \u001b[33m'\u001b[39m\u001b[33msearch\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    229\u001b[39m     q = \u001b[33m'\u001b[39m\u001b[33mHow do I configure logfire to work with FastAPI?\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m run_agent(q)\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    232\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mUnknown action: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mrun_agent\u001b[39m\u001b[34m(question)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m database_connect(\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[32m     74\u001b[39m     deps = Deps(openai=openai, pool=pool)\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     answer = \u001b[38;5;28;01mawait\u001b[39;00m agent.run(question, deps=deps)\n\u001b[32m     76\u001b[39m \u001b[38;5;28mprint\u001b[39m(answer.output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pydantic_ai/agent.py:451\u001b[39m, in \u001b[36mAgent.run\u001b[39m\u001b[34m(self, user_prompt, output_type, message_history, model, deps, model_settings, usage_limits, usage, infer_name, **_deprecated_kwargs)\u001b[39m\n\u001b[32m    439\u001b[39m     output_type = _deprecated_kwargs[\u001b[33m'\u001b[39m\u001b[33mresult_type\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter(\n\u001b[32m    442\u001b[39m     user_prompt=user_prompt,\n\u001b[32m    443\u001b[39m     output_type=output_type,\n\u001b[32m   (...)\u001b[39m\u001b[32m    449\u001b[39m     usage=usage,\n\u001b[32m    450\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m agent_run:\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m agent_run:\n\u001b[32m    452\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m agent_run.result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m'\u001b[39m\u001b[33mThe graph run did not finish properly\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pydantic_ai/agent.py:1797\u001b[39m, in \u001b[36mAgentRun.__anext__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1793\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__anext__\u001b[39m(\n\u001b[32m   1794\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1795\u001b[39m ) -> _agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]:\n\u001b[32m   1796\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Advance to the next node automatically based on the last returned node.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1797\u001b[39m     next_node = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._graph_run.\u001b[34m__anext__\u001b[39m()\n\u001b[32m   1798\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _agent_graph.is_agent_node(next_node):\n\u001b[32m   1799\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m next_node\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pydantic_graph/graph.py:810\u001b[39m, in \u001b[36mGraphRun.__anext__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    807\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._next_node, End):\n\u001b[32m    808\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m810\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.next(\u001b[38;5;28mself\u001b[39m._next_node)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pydantic_graph/graph.py:783\u001b[39m, in \u001b[36mGraphRun.next\u001b[39m\u001b[34m(self, node)\u001b[39m\n\u001b[32m    781\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.persistence.record_run(node_snapshot_id):\n\u001b[32m    782\u001b[39m         ctx = GraphRunContext(\u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.deps)\n\u001b[32m--> \u001b[39m\u001b[32m783\u001b[39m         \u001b[38;5;28mself\u001b[39m._next_node = \u001b[38;5;28;01mawait\u001b[39;00m node.run(ctx)\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._next_node, End):\n\u001b[32m    786\u001b[39m     \u001b[38;5;28mself\u001b[39m._snapshot_id = \u001b[38;5;28mself\u001b[39m._next_node.get_snapshot_id()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py:380\u001b[39m, in \u001b[36mCallToolsNode.run\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\n\u001b[32m    378\u001b[39m     \u001b[38;5;28mself\u001b[39m, ctx: GraphRunContext[GraphAgentState, GraphAgentDeps[DepsT, NodeRunEndT]]\n\u001b[32m    379\u001b[39m ) -> Union[ModelRequestNode[DepsT, NodeRunEndT], End[result.FinalResult[NodeRunEndT]]]:  \u001b[38;5;66;03m# noqa UP007\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream(ctx):\n\u001b[32m    381\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    382\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._next_node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m'\u001b[39m\u001b[33mthe stream should set `self._next_node` before it ends\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/contextlib.py:217\u001b[39m, in \u001b[36m_AsyncGeneratorContextManager.__aexit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m anext(\u001b[38;5;28mself\u001b[39m.gen)\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[32m    219\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py:394\u001b[39m, in \u001b[36mCallToolsNode.stream\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m stream\n\u001b[32m    393\u001b[39m \u001b[38;5;66;03m# Run the stream to completion if it was not finished:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _event \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m    395\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py:443\u001b[39m, in \u001b[36mCallToolsNode._run_stream\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m    439\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m exceptions.UnexpectedModelBehavior(\u001b[33m'\u001b[39m\u001b[33mReceived empty model response\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    441\u001b[39m     \u001b[38;5;28mself\u001b[39m._events_iterator = _run_stream()\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._events_iterator:\n\u001b[32m    444\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m event\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py:421\u001b[39m, in \u001b[36mCallToolsNode._run_stream.<locals>._run_stream\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    416\u001b[39m \u001b[38;5;66;03m# At the moment, we prioritize at least executing tool calls if they are present.\u001b[39;00m\n\u001b[32m    417\u001b[39m \u001b[38;5;66;03m# In the future, we'd consider making this configurable at the agent or run level.\u001b[39;00m\n\u001b[32m    418\u001b[39m \u001b[38;5;66;03m# This accounts for cases like anthropic returns that might contain a text response\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;66;03m# and a tool call response, where the text response just indicates the tool call will happen.\u001b[39;00m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tool_calls:\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._handle_tool_calls(ctx, tool_calls):\n\u001b[32m    422\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m event\n\u001b[32m    423\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m texts:\n\u001b[32m    424\u001b[39m     \u001b[38;5;66;03m# No events are emitted during the handling of text responses, so we don't need to yield anything\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py:472\u001b[39m, in \u001b[36mCallToolsNode._handle_tool_calls\u001b[39m\u001b[34m(self, ctx, tool_calls)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;66;03m# Then build the other request parts based on end strategy\u001b[39;00m\n\u001b[32m    471\u001b[39m tool_responses: \u001b[38;5;28mlist\u001b[39m[_messages.ModelRequestPart] = \u001b[38;5;28mself\u001b[39m._tool_responses\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m process_function_tools(\n\u001b[32m    473\u001b[39m     tool_calls,\n\u001b[32m    474\u001b[39m     final_result \u001b[38;5;129;01mand\u001b[39;00m final_result.tool_name,\n\u001b[32m    475\u001b[39m     final_result \u001b[38;5;129;01mand\u001b[39;00m final_result.tool_call_id,\n\u001b[32m    476\u001b[39m     ctx,\n\u001b[32m    477\u001b[39m     tool_responses,\n\u001b[32m    478\u001b[39m ):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m event\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m final_result:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py:658\u001b[39m, in \u001b[36mprocess_function_tools\u001b[39m\u001b[34m(tool_calls, output_tool_name, output_tool_call_id, ctx, output_parts)\u001b[39m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m done:\n\u001b[32m    657\u001b[39m     index = tasks.index(task)\n\u001b[32m--> \u001b[39m\u001b[32m658\u001b[39m     result = \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m _messages.FunctionToolResultEvent(result, tool_call_id=call_index_to_event_id[index])\n\u001b[32m    661\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, _messages.RetryPromptPart):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pydantic_ai/tools.py:329\u001b[39m, in \u001b[36mTool.run\u001b[39m\u001b[34m(self, message, run_context, tracer)\u001b[39m\n\u001b[32m    310\u001b[39m span_attributes = {\n\u001b[32m    311\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mgen_ai.tool.name\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.name,\n\u001b[32m    312\u001b[39m     \u001b[38;5;66;03m# NOTE: this means `gen_ai.tool.call.id` will be included even if it was generated by pydantic-ai\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    326\u001b[39m     ),\n\u001b[32m    327\u001b[39m }\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tracer.start_as_current_span(\u001b[33m'\u001b[39m\u001b[33mrunning tool\u001b[39m\u001b[33m'\u001b[39m, attributes=span_attributes):\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run(message, run_context)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pydantic_ai/tools.py:346\u001b[39m, in \u001b[36mTool._run\u001b[39m\u001b[34m(self, message, run_context)\u001b[39m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_async:\n\u001b[32m    345\u001b[39m     function = cast(Callable[[Any], Awaitable[\u001b[38;5;28mstr\u001b[39m]], \u001b[38;5;28mself\u001b[39m.function)\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     response_content = \u001b[38;5;28;01mawait\u001b[39;00m function(*args, **kwargs)\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    348\u001b[39m     function = cast(Callable[[Any], \u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mself\u001b[39m.function)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mretrieve\u001b[39m\u001b[34m(context, search_query)\u001b[39m\n\u001b[32m     54\u001b[39m embedding = embedding.data[\u001b[32m0\u001b[39m].embedding\n\u001b[32m     55\u001b[39m embedding_json = pydantic_core.to_json(embedding).decode()\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m rows = \u001b[38;5;28;01mawait\u001b[39;00m context.deps.pool.fetch(\n\u001b[32m     57\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mSELECT url, title, content FROM doc_sections ORDER BY embedding <-> $1 LIMIT 8\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     58\u001b[39m     embedding_json,\n\u001b[32m     59\u001b[39m )\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m.join(\n\u001b[32m     61\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m# \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[33m\"\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDocumentation URL:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[33m\"\u001b[39m\u001b[33murl\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rows\n\u001b[32m     63\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/asyncpg/pool.py:570\u001b[39m, in \u001b[36mPool.fetch\u001b[39m\u001b[34m(self, query, timeout, record_class, *args)\u001b[39m\n\u001b[32m    561\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run a query and return the results as a list of :class:`Record`.\u001b[39;00m\n\u001b[32m    562\u001b[39m \n\u001b[32m    563\u001b[39m \u001b[33;03mPool performs this operation using one of its connections.  Other than\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    567\u001b[39m \u001b[33;03m.. versionadded:: 0.10.0\u001b[39;00m\n\u001b[32m    568\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    569\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.acquire() \u001b[38;5;28;01mas\u001b[39;00m con:\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m con.fetch(\n\u001b[32m    571\u001b[39m         query,\n\u001b[32m    572\u001b[39m         *args,\n\u001b[32m    573\u001b[39m         timeout=timeout,\n\u001b[32m    574\u001b[39m         record_class=record_class\n\u001b[32m    575\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/opentelemetry/instrumentation/asyncpg/__init__.py:177\u001b[39m, in \u001b[36mAsyncPGInstrumentor._do_execute\u001b[39m\u001b[34m(self, func, instance, args, kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         span.set_attribute(attribute, value)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:  \u001b[38;5;66;03m# pylint: disable=W0703\u001b[39;00m\n\u001b[32m    179\u001b[39m     exception = exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/asyncpg/connection.py:691\u001b[39m, in \u001b[36mConnection.fetch\u001b[39m\u001b[34m(self, query, timeout, record_class, *args)\u001b[39m\n\u001b[32m    670\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run a query and return the results as a list of :class:`Record`.\u001b[39;00m\n\u001b[32m    671\u001b[39m \n\u001b[32m    672\u001b[39m \u001b[33;03m:param str query:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    688\u001b[39m \u001b[33;03m    Added the *record_class* parameter.\u001b[39;00m\n\u001b[32m    689\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    690\u001b[39m \u001b[38;5;28mself\u001b[39m._check_open()\n\u001b[32m--> \u001b[39m\u001b[32m691\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._execute(\n\u001b[32m    692\u001b[39m     query,\n\u001b[32m    693\u001b[39m     args,\n\u001b[32m    694\u001b[39m     \u001b[32m0\u001b[39m,\n\u001b[32m    695\u001b[39m     timeout,\n\u001b[32m    696\u001b[39m     record_class=record_class,\n\u001b[32m    697\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/asyncpg/connection.py:1794\u001b[39m, in \u001b[36mConnection._execute\u001b[39m\u001b[34m(self, query, args, limit, timeout, return_status, ignore_custom_codec, record_class)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_execute\u001b[39m(\n\u001b[32m   1783\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1784\u001b[39m     query,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1791\u001b[39m     record_class=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1792\u001b[39m ):\n\u001b[32m   1793\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stmt_exclusive_section:\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m         result, _ = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__execute(\n\u001b[32m   1795\u001b[39m             query,\n\u001b[32m   1796\u001b[39m             args,\n\u001b[32m   1797\u001b[39m             limit,\n\u001b[32m   1798\u001b[39m             timeout,\n\u001b[32m   1799\u001b[39m             return_status=return_status,\n\u001b[32m   1800\u001b[39m             record_class=record_class,\n\u001b[32m   1801\u001b[39m             ignore_custom_codec=ignore_custom_codec,\n\u001b[32m   1802\u001b[39m         )\n\u001b[32m   1803\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/asyncpg/connection.py:1892\u001b[39m, in \u001b[36mConnection.__execute\u001b[39m\u001b[34m(self, query, args, limit, timeout, return_status, ignore_custom_codec, record_class)\u001b[39m\n\u001b[32m   1884\u001b[39m         result, stmt = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._do_execute(\n\u001b[32m   1885\u001b[39m             query,\n\u001b[32m   1886\u001b[39m             executor,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1889\u001b[39m             ignore_custom_codec=ignore_custom_codec,\n\u001b[32m   1890\u001b[39m         )\n\u001b[32m   1891\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1892\u001b[39m     result, stmt = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._do_execute(\n\u001b[32m   1893\u001b[39m         query,\n\u001b[32m   1894\u001b[39m         executor,\n\u001b[32m   1895\u001b[39m         timeout,\n\u001b[32m   1896\u001b[39m         record_class=record_class,\n\u001b[32m   1897\u001b[39m         ignore_custom_codec=ignore_custom_codec,\n\u001b[32m   1898\u001b[39m     )\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result, stmt\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/asyncpg/connection.py:1925\u001b[39m, in \u001b[36mConnection._do_execute\u001b[39m\u001b[34m(self, query, executor, timeout, retry, ignore_custom_codec, record_class)\u001b[39m\n\u001b[32m   1914\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_do_execute\u001b[39m(\n\u001b[32m   1915\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1916\u001b[39m     query,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1922\u001b[39m     record_class=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1923\u001b[39m ):\n\u001b[32m   1924\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1925\u001b[39m         stmt = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_statement(\n\u001b[32m   1926\u001b[39m             query,\n\u001b[32m   1927\u001b[39m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1928\u001b[39m             record_class=record_class,\n\u001b[32m   1929\u001b[39m             ignore_custom_codec=ignore_custom_codec,\n\u001b[32m   1930\u001b[39m         )\n\u001b[32m   1931\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1932\u001b[39m         before = time.monotonic()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/asyncpg/connection.py:433\u001b[39m, in \u001b[36mConnection._get_statement\u001b[39m\u001b[34m(self, query, timeout, named, use_cache, ignore_custom_codec, record_class)\u001b[39m\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    431\u001b[39m     stmt_name = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m statement = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.prepare(\n\u001b[32m    434\u001b[39m     stmt_name,\n\u001b[32m    435\u001b[39m     query,\n\u001b[32m    436\u001b[39m     timeout,\n\u001b[32m    437\u001b[39m     record_class=record_class,\n\u001b[32m    438\u001b[39m     ignore_custom_codec=ignore_custom_codec,\n\u001b[32m    439\u001b[39m )\n\u001b[32m    440\u001b[39m need_reprepare = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    441\u001b[39m types_with_missing_codecs = statement._init_types()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/asyncpg/protocol/protocol.pyx:166\u001b[39m, in \u001b[36mprepare\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mUndefinedTableError\u001b[39m: relation \"doc_sections\" does not exist"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations as _annotations\n",
    "\n",
    "import asyncio\n",
    "import re\n",
    "import sys\n",
    "import unicodedata\n",
    "from contextlib import asynccontextmanager\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import asyncpg\n",
    "import httpx\n",
    "import logfire\n",
    "import pydantic_core\n",
    "from openai import AsyncOpenAI\n",
    "from pydantic import TypeAdapter\n",
    "from typing_extensions import AsyncGenerator\n",
    "\n",
    "from pydantic_ai import RunContext\n",
    "from pydantic_ai.agent import Agent\n",
    "\n",
    "# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\n",
    "logfire.configure(send_to_logfire='if-token-present')\n",
    "logfire.instrument_asyncpg()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Deps:\n",
    "    openai: AsyncOpenAI\n",
    "    pool: asyncpg.Pool\n",
    "\n",
    "\n",
    "agent = Agent(pydantic_model, deps_type=Deps, instrument=True)\n",
    "\n",
    "\n",
    "@agent.tool\n",
    "async def retrieve(context: RunContext[Deps], search_query: str) -> str:\n",
    "    \"\"\"Retrieve documentation sections based on a search query.\n",
    "\n",
    "    Args:\n",
    "        context: The call context.\n",
    "        search_query: The search query.\n",
    "    \"\"\"\n",
    "    with logfire.span(\n",
    "        'create embedding for {search_query=}', search_query=search_query\n",
    "    ):\n",
    "        embedding = await context.deps.openai.embeddings.create(\n",
    "            input=search_query,\n",
    "            model='text-embedding-3-small',\n",
    "        )\n",
    "\n",
    "    assert len(embedding.data) == 1, (\n",
    "        f'Expected 1 embedding, got {len(embedding.data)}, doc query: {search_query!r}'\n",
    "    )\n",
    "    embedding = embedding.data[0].embedding\n",
    "    embedding_json = pydantic_core.to_json(embedding).decode()\n",
    "    rows = await context.deps.pool.fetch(\n",
    "        'SELECT url, title, content FROM doc_sections ORDER BY embedding <-> $1 LIMIT 8',\n",
    "        embedding_json,\n",
    "    )\n",
    "    return '\\n\\n'.join(\n",
    "        f'# {row[\"title\"]}\\nDocumentation URL:{row[\"url\"]}\\n\\n{row[\"content\"]}\\n'\n",
    "        for row in rows\n",
    "    )\n",
    "\n",
    "\n",
    "async def run_agent(question: str):\n",
    "    \"\"\"Entry point to run the agent and perform RAG based question answering.\"\"\"\n",
    "    openai = AsyncOpenAI()\n",
    "    logfire.instrument_openai(openai)\n",
    "\n",
    "    logfire.info('Asking \"{question}\"', question=question)\n",
    "\n",
    "    async with database_connect(True) as pool:\n",
    "        deps = Deps(openai=openai, pool=pool)\n",
    "        answer = await agent.run(question, deps=deps)\n",
    "    print(answer.output)\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# The rest of this file is dedicated to preparing the #\n",
    "# search database, and some utilities.                #\n",
    "#######################################################\n",
    "\n",
    "# JSON document from\n",
    "# https://gist.github.com/samuelcolvin/4b5bb9bb163b1122ff17e29e48c10992\n",
    "DOCS_JSON = (\n",
    "    'https://gist.githubusercontent.com/'\n",
    "    'samuelcolvin/4b5bb9bb163b1122ff17e29e48c10992/raw/'\n",
    "    '80c5925c42f1442c24963aaf5eb1a324d47afe95/logfire_docs.json'\n",
    ")\n",
    "\n",
    "\n",
    "async def build_search_db():\n",
    "    \"\"\"Build the search database.\"\"\"\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response = await client.get(DOCS_JSON)\n",
    "        response.raise_for_status()\n",
    "    sections = sessions_ta.validate_json(response.content)\n",
    "\n",
    "    openai = AsyncOpenAI()\n",
    "    logfire.instrument_openai(openai)\n",
    "\n",
    "    async with database_connect(True) as pool:\n",
    "        with logfire.span('create schema'):\n",
    "            async with pool.acquire() as conn:\n",
    "                async with conn.transaction():\n",
    "                    await conn.execute(DB_SCHEMA)\n",
    "\n",
    "        sem = asyncio.Semaphore(10)\n",
    "        async with asyncio.TaskGroup() as tg:\n",
    "            for section in sections:\n",
    "                tg.create_task(insert_doc_section(sem, openai, pool, section))\n",
    "\n",
    "\n",
    "async def insert_doc_section(\n",
    "    sem: asyncio.Semaphore,\n",
    "    openai: AsyncOpenAI,\n",
    "    pool: asyncpg.Pool,\n",
    "    section: DocsSection,\n",
    ") -> None:\n",
    "    async with sem:\n",
    "        url = section.url()\n",
    "        exists = await pool.fetchval('SELECT 1 FROM doc_sections WHERE url = $1', url)\n",
    "        if exists:\n",
    "            logfire.info('Skipping {url=}', url=url)\n",
    "            return\n",
    "\n",
    "        with logfire.span('create embedding for {url=}', url=url):\n",
    "            embedding = await openai.embeddings.create(\n",
    "                input=section.embedding_content(),\n",
    "                model='text-embedding-3-small',\n",
    "            )\n",
    "        assert len(embedding.data) == 1, (\n",
    "            f'Expected 1 embedding, got {len(embedding.data)}, doc section: {section}'\n",
    "        )\n",
    "        embedding = embedding.data[0].embedding\n",
    "        embedding_json = pydantic_core.to_json(embedding).decode()\n",
    "        await pool.execute(\n",
    "            'INSERT INTO doc_sections (url, title, content, embedding) VALUES ($1, $2, $3, $4)',\n",
    "            url,\n",
    "            section.title,\n",
    "            section.content,\n",
    "            embedding_json,\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DocsSection:\n",
    "    id: int\n",
    "    parent: int | None\n",
    "    path: str\n",
    "    level: int\n",
    "    title: str\n",
    "    content: str\n",
    "\n",
    "    def url(self) -> str:\n",
    "        url_path = re.sub(r'\\.md$', '', self.path)\n",
    "        return (\n",
    "            f'https://logfire.pydantic.dev/docs/{url_path}/#{slugify(self.title, \"-\")}'\n",
    "        )\n",
    "\n",
    "    def embedding_content(self) -> str:\n",
    "        return '\\n\\n'.join((f'path: {self.path}', f'title: {self.title}', self.content))\n",
    "\n",
    "\n",
    "sessions_ta = TypeAdapter(list[DocsSection])\n",
    "\n",
    "\n",
    "# pyright: reportUnknownMemberType=false\n",
    "# pyright: reportUnknownVariableType=false\n",
    "@asynccontextmanager\n",
    "async def database_connect(\n",
    "    create_db: bool = False,\n",
    ") -> AsyncGenerator[asyncpg.Pool, None]:\n",
    "    server_dsn, database = (\n",
    "        'postgresql://postgres:postgres@localhost:54320',\n",
    "        'pydantic_ai_rag',\n",
    "    )\n",
    "    if create_db:\n",
    "        with logfire.span('check and create DB'):\n",
    "            conn = await asyncpg.connect(server_dsn)\n",
    "            try:\n",
    "                db_exists = await conn.fetchval(\n",
    "                    'SELECT 1 FROM pg_database WHERE datname = $1', database\n",
    "                )\n",
    "                if not db_exists:\n",
    "                    await conn.execute(f'CREATE DATABASE {database}')\n",
    "            finally:\n",
    "                await conn.close()\n",
    "\n",
    "    pool = await asyncpg.create_pool(f'{server_dsn}/{database}')\n",
    "    try:\n",
    "        yield pool\n",
    "    finally:\n",
    "        await pool.close()\n",
    "\n",
    "\n",
    "DB_SCHEMA = \"\"\"\n",
    "CREATE EXTENSION IF NOT EXISTS vector;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS doc_sections (\n",
    "    id serial PRIMARY KEY,\n",
    "    url text NOT NULL UNIQUE,\n",
    "    title text NOT NULL,\n",
    "    content text NOT NULL,\n",
    "    -- text-embedding-3-small returns a vector of 1536 floats\n",
    "    embedding vector(1536) NOT NULL\n",
    ");\n",
    "CREATE INDEX IF NOT EXISTS idx_doc_sections_embedding ON doc_sections USING hnsw (embedding vector_l2_ops);\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def slugify(value: str, separator: str, unicode: bool = False) -> str:\n",
    "    \"\"\"Slugify a string, to make it URL friendly.\"\"\"\n",
    "    # Taken unchanged from https://github.com/Python-Markdown/markdown/blob/3.7/markdown/extensions/toc.py#L38\n",
    "    if not unicode:\n",
    "        # Replace Extended Latin characters with ASCII, i.e. `žlutý` => `zluty`\n",
    "        value = unicodedata.normalize('NFKD', value)\n",
    "        value = value.encode('ascii', 'ignore').decode('ascii')\n",
    "    value = re.sub(r'[^\\w\\s-]', '', value).strip().lower()\n",
    "    return re.sub(rf'[{separator}\\s]+', separator, value)\n",
    "\n",
    "\n",
    "action = 'search'\n",
    "\n",
    "if action == 'build':\n",
    "    await build_search_db()\n",
    "elif action == 'search':\n",
    "    q = 'How do I configure logfire to work with FastAPI?'\n",
    "    await run_agent(q)\n",
    "else:\n",
    "    print(f'Unknown action: {action!r}')\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5d4812ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:42:04.792 search_agent run\n",
      "12:42:04.796   chat gpt-4o-mini\n",
      "12:42:06.164   running 1 tool\n",
      "12:42:06.164     running tool: extract_flights\n",
      "12:42:11.206       found 8 flights\n",
      "12:42:11.208   chat gpt-4o-mini\n",
      "Flight found: flight_number='LA101' price=250 origin='SFO' destination='ANC' date=datetime.date(2025, 1, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Do you want to buy this flight, or keep searching? (buy/*search): </pre>\n"
      ],
      "text/plain": [
       "Do you want to buy this flight, or keep searching? (buy/*search): "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">Please select one of the available options</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31mPlease select one of the available options\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Do you want to buy this flight, or keep searching? (buy/*search): </pre>\n"
      ],
      "text/plain": [
       "Do you want to buy this flight, or keep searching? (buy/*search): "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:42:27.701 search_agent run\n",
      "12:42:27.702   chat gpt-4o-mini\n",
      "Flight found: flight_number='LA101' price=250 origin='SFO' destination='ANC' date=datetime.date(2025, 1, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Do you want to buy this flight, or keep searching? (buy/*search): </pre>\n"
      ],
      "text/plain": [
       "Do you want to buy this flight, or keep searching? (buy/*search): "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">What seat would you like?: </pre>\n"
      ],
      "text/plain": [
       "What seat would you like?: "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purchasing flight flight_details=FlightDetails(flight_number='LA101', price=250, origin='SFO', destination='ANC', date=datetime.date(2025, 1, 10)) seat=SeatPreference(row=1, seat='A')...\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from dataclasses import dataclass\n",
    "from typing import Literal\n",
    "\n",
    "import logfire\n",
    "from pydantic import BaseModel, Field\n",
    "from rich.prompt import Prompt\n",
    "\n",
    "from pydantic_ai import Agent, ModelRetry, RunContext\n",
    "from pydantic_ai.messages import ModelMessage\n",
    "from pydantic_ai.usage import Usage, UsageLimits\n",
    "\n",
    "# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\n",
    "logfire.configure(send_to_logfire='if-token-present')\n",
    "\n",
    "\n",
    "class FlightDetails(BaseModel):\n",
    "    \"\"\"Details of the most suitable flight.\"\"\"\n",
    "\n",
    "    flight_number: str\n",
    "    price: int\n",
    "    origin: str = Field(description='Three-letter airport code')\n",
    "    destination: str = Field(description='Three-letter airport code')\n",
    "    date: datetime.date\n",
    "\n",
    "\n",
    "class NoFlightFound(BaseModel):\n",
    "    \"\"\"When no valid flight is found.\"\"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Deps:\n",
    "    web_page_text: str\n",
    "    req_origin: str\n",
    "    req_destination: str\n",
    "    req_date: datetime.date\n",
    "\n",
    "\n",
    "# This agent is responsible for controlling the flow of the conversation.\n",
    "search_agent = Agent[Deps, FlightDetails | NoFlightFound](\n",
    "    f'openai:{model_name}',\n",
    "    output_type=FlightDetails | NoFlightFound,  # type: ignore\n",
    "    retries=4,\n",
    "    system_prompt=(\n",
    "        'Your job is to find the cheapest flight for the user on the given date. '\n",
    "    ),\n",
    "    instrument=True,\n",
    ")\n",
    "\n",
    "\n",
    "# This agent is responsible for extracting flight details from web page text.\n",
    "extraction_agent = Agent(\n",
    "    f'openai:{model_name}',\n",
    "    output_type=list[FlightDetails],\n",
    "    system_prompt='Extract all the flight details from the given text.',\n",
    ")\n",
    "\n",
    "\n",
    "@search_agent.tool\n",
    "async def extract_flights(ctx: RunContext[Deps]) -> list[FlightDetails]:\n",
    "    \"\"\"Get details of all flights.\"\"\"\n",
    "    # we pass the usage to the search agent so requests within this agent are counted\n",
    "    result = await extraction_agent.run(ctx.deps.web_page_text, usage=ctx.usage)\n",
    "    logfire.info('found {flight_count} flights', flight_count=len(result.output))\n",
    "    return result.output\n",
    "\n",
    "\n",
    "@search_agent.output_validator\n",
    "async def validate_output(\n",
    "    ctx: RunContext[Deps], output: FlightDetails | NoFlightFound\n",
    ") -> FlightDetails | NoFlightFound:\n",
    "    \"\"\"Procedural validation that the flight meets the constraints.\"\"\"\n",
    "    if isinstance(output, NoFlightFound):\n",
    "        return output\n",
    "\n",
    "    errors: list[str] = []\n",
    "    if output.origin != ctx.deps.req_origin:\n",
    "        errors.append(\n",
    "            f'Flight should have origin {ctx.deps.req_origin}, not {output.origin}'\n",
    "        )\n",
    "    if output.destination != ctx.deps.req_destination:\n",
    "        errors.append(\n",
    "            f'Flight should have destination {ctx.deps.req_destination}, not {output.destination}'\n",
    "        )\n",
    "    if output.date != ctx.deps.req_date:\n",
    "        errors.append(f'Flight should be on {ctx.deps.req_date}, not {output.date}')\n",
    "\n",
    "    if errors:\n",
    "        raise ModelRetry('\\n'.join(errors))\n",
    "    else:\n",
    "        return output\n",
    "\n",
    "\n",
    "class SeatPreference(BaseModel):\n",
    "    row: int = Field(ge=1, le=30)\n",
    "    seat: Literal['A', 'B', 'C', 'D', 'E', 'F']\n",
    "\n",
    "\n",
    "class Failed(BaseModel):\n",
    "    \"\"\"Unable to extract a seat selection.\"\"\"\n",
    "\n",
    "\n",
    "# This agent is responsible for extracting the user's seat selection\n",
    "seat_preference_agent = Agent[None, SeatPreference | Failed](\n",
    "    f'openai:{model_name}',\n",
    "    output_type=SeatPreference | Failed,  # type: ignore\n",
    "    system_prompt=(\n",
    "        \"Extract the user's seat preference. \"\n",
    "        'Seats A and F are window seats. '\n",
    "        'Row 1 is the front row and has extra leg room. '\n",
    "        'Rows 14, and 20 also have extra leg room. '\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "# in reality this would be downloaded from a booking site,\n",
    "# potentially using another agent to navigate the site\n",
    "flights_web_page = \"\"\"\n",
    "1. Flight SFO-AK123\n",
    "- Price: $350\n",
    "- Origin: San Francisco International Airport (SFO)\n",
    "- Destination: Ted Stevens Anchorage International Airport (ANC)\n",
    "- Date: January 10, 2025\n",
    "\n",
    "2. Flight SFO-AK456\n",
    "- Price: $370\n",
    "- Origin: San Francisco International Airport (SFO)\n",
    "- Destination: Fairbanks International Airport (FAI)\n",
    "- Date: January 10, 2025\n",
    "\n",
    "3. Flight SFO-AK789\n",
    "- Price: $400\n",
    "- Origin: San Francisco International Airport (SFO)\n",
    "- Destination: Juneau International Airport (JNU)\n",
    "- Date: January 20, 2025\n",
    "\n",
    "4. Flight NYC-LA101\n",
    "- Price: $250\n",
    "- Origin: San Francisco International Airport (SFO)\n",
    "- Destination: Ted Stevens Anchorage International Airport (ANC)\n",
    "- Date: January 10, 2025\n",
    "\n",
    "5. Flight CHI-MIA202\n",
    "- Price: $200\n",
    "- Origin: Chicago O'Hare International Airport (ORD)\n",
    "- Destination: Miami International Airport (MIA)\n",
    "- Date: January 12, 2025\n",
    "\n",
    "6. Flight BOS-SEA303\n",
    "- Price: $120\n",
    "- Origin: Boston Logan International Airport (BOS)\n",
    "- Destination: Ted Stevens Anchorage International Airport (ANC)\n",
    "- Date: January 12, 2025\n",
    "\n",
    "7. Flight DFW-DEN404\n",
    "- Price: $150\n",
    "- Origin: Dallas/Fort Worth International Airport (DFW)\n",
    "- Destination: Denver International Airport (DEN)\n",
    "- Date: January 10, 2025\n",
    "\n",
    "8. Flight ATL-HOU505\n",
    "- Price: $180\n",
    "- Origin: Hartsfield-Jackson Atlanta International Airport (ATL)\n",
    "- Destination: George Bush Intercontinental Airport (IAH)\n",
    "- Date: January 10, 2025\n",
    "\"\"\"\n",
    "\n",
    "# restrict how many requests this app can make to the LLM\n",
    "usage_limits = UsageLimits(request_limit=15)\n",
    "\n",
    "\n",
    "async def find_seat(usage: Usage) -> SeatPreference:\n",
    "    message_history: list[ModelMessage] | None = None\n",
    "    while True:\n",
    "        answer = Prompt.ask('What seat would you like?')\n",
    "\n",
    "        result = await seat_preference_agent.run(\n",
    "            answer,\n",
    "            message_history=message_history,\n",
    "            usage=usage,\n",
    "            usage_limits=usage_limits,\n",
    "        )\n",
    "        if isinstance(result.output, SeatPreference):\n",
    "            return result.output\n",
    "        else:\n",
    "            print('Could not understand seat preference. Please try again.')\n",
    "            message_history = result.all_messages()\n",
    "\n",
    "\n",
    "async def buy_tickets(flight_details: FlightDetails, seat: SeatPreference):\n",
    "    print(f'Purchasing flight {flight_details=!r} {seat=!r}...')\n",
    "\n",
    "async def main():\n",
    "    deps = Deps(\n",
    "        web_page_text=flights_web_page,\n",
    "        req_origin='SFO',\n",
    "        req_destination='ANC',\n",
    "        req_date=datetime.date(2025, 1, 10),\n",
    "    )\n",
    "    message_history: list[ModelMessage] | None = None\n",
    "    usage: Usage = Usage()\n",
    "    # run the agent until a satisfactory flight is found\n",
    "    while True:\n",
    "        result = await search_agent.run(\n",
    "            f'Find me a flight from {deps.req_origin} to {deps.req_destination} on {deps.req_date}',\n",
    "            deps=deps,\n",
    "            usage=usage,\n",
    "            message_history=message_history,\n",
    "            usage_limits=usage_limits,\n",
    "        )\n",
    "        if isinstance(result.output, NoFlightFound):\n",
    "            print('No flight found')\n",
    "            break\n",
    "        else:\n",
    "            flight = result.output\n",
    "            print(f'Flight found: {flight}')\n",
    "            answer = Prompt.ask(\n",
    "                'Do you want to buy this flight, or keep searching? (buy/*search)',\n",
    "                choices=['buy', 'search', ''],\n",
    "                show_choices=False,\n",
    "            )\n",
    "            if answer == 'buy':\n",
    "                seat = await find_seat(usage)\n",
    "                await buy_tickets(flight, seat)\n",
    "                break\n",
    "            else:\n",
    "                message_history = result.all_messages(\n",
    "                    output_tool_return_content='Please suggest another flight'\n",
    "                )\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accfffa4",
   "metadata": {},
   "outputs": [
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 113\u001b[39m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28mprint\u001b[39m(result.output)\n\u001b[32m    106\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[33;03m    Email(\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[33;03m        subject='Welcome to our tech blog!',\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[33;03m        body='Hello John, Welcome to our tech blog! ...',\u001b[39;00m\n\u001b[32m    110\u001b[39m \u001b[33;03m    )\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 104\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    102\u001b[39m state = State(user)\n\u001b[32m    103\u001b[39m feedback_graph = Graph(nodes=(WriteEmail, Feedback))\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m feedback_graph.run(WriteEmail(), state=state)\n\u001b[32m    105\u001b[39m \u001b[38;5;28mprint\u001b[39m(result.output)\n\u001b[32m    106\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[33;03mEmail(\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[33;03m    subject='Welcome to our tech blog!',\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[33;03m    body='Hello John, Welcome to our tech blog! ...',\u001b[39;00m\n\u001b[32m    110\u001b[39m \u001b[33;03m)\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pydantic_graph/graph.py:167\u001b[39m, in \u001b[36mGraph.run\u001b[39m\u001b[34m(self, start_node, state, deps, persistence, infer_name)\u001b[39m\n\u001b[32m    162\u001b[39m     \u001b[38;5;28mself\u001b[39m._infer_name(inspect.currentframe())\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter(\n\u001b[32m    165\u001b[39m     start_node, state=state, deps=deps, persistence=persistence, infer_name=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    166\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m graph_run:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _node \u001b[38;5;129;01min\u001b[39;00m graph_run:\n\u001b[32m    168\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    170\u001b[39m result = graph_run.result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pydantic_graph/graph.py:810\u001b[39m, in \u001b[36mGraphRun.__anext__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    807\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._next_node, End):\n\u001b[32m    808\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m810\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.next(\u001b[38;5;28mself\u001b[39m._next_node)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pydantic_graph/graph.py:783\u001b[39m, in \u001b[36mGraphRun.next\u001b[39m\u001b[34m(self, node)\u001b[39m\n\u001b[32m    781\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.persistence.record_run(node_snapshot_id):\n\u001b[32m    782\u001b[39m         ctx = GraphRunContext(\u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.deps)\n\u001b[32m--> \u001b[39m\u001b[32m783\u001b[39m         \u001b[38;5;28mself\u001b[39m._next_node = \u001b[38;5;28;01mawait\u001b[39;00m node.run(ctx)\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._next_node, End):\n\u001b[32m    786\u001b[39m     \u001b[38;5;28mself\u001b[39m._snapshot_id = \u001b[38;5;28mself\u001b[39m._next_node.get_snapshot_id()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mWriteEmail.run\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     50\u001b[39m     prompt = (\n\u001b[32m     51\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mWrite a welcome email for the user:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     52\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformat_as_xml(ctx.state.user)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     53\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m email_writer_agent.run(\n\u001b[32m     56\u001b[39m     prompt,\n\u001b[32m     57\u001b[39m     message_history=ctx.state.write_agent_messages,\n\u001b[32m     58\u001b[39m )\n\u001b[32m     59\u001b[39m ctx.state.write_agent_messages += result.new_messages()\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Feedback(result.output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pydantic_ai/agent.py:451\u001b[39m, in \u001b[36mAgent.run\u001b[39m\u001b[34m(self, user_prompt, output_type, message_history, model, deps, model_settings, usage_limits, usage, infer_name, **_deprecated_kwargs)\u001b[39m\n\u001b[32m    439\u001b[39m     output_type = _deprecated_kwargs[\u001b[33m'\u001b[39m\u001b[33mresult_type\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter(\n\u001b[32m    442\u001b[39m     user_prompt=user_prompt,\n\u001b[32m    443\u001b[39m     output_type=output_type,\n\u001b[32m   (...)\u001b[39m\u001b[32m    449\u001b[39m     usage=usage,\n\u001b[32m    450\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m agent_run:\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m agent_run:\n\u001b[32m    452\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m agent_run.result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m'\u001b[39m\u001b[33mThe graph run did not finish properly\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pydantic_ai/agent.py:1797\u001b[39m, in \u001b[36mAgentRun.__anext__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1793\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__anext__\u001b[39m(\n\u001b[32m   1794\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1795\u001b[39m ) -> _agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]:\n\u001b[32m   1796\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Advance to the next node automatically based on the last returned node.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1797\u001b[39m     next_node = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._graph_run.\u001b[34m__anext__\u001b[39m()\n\u001b[32m   1798\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _agent_graph.is_agent_node(next_node):\n\u001b[32m   1799\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m next_node\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pydantic_graph/graph.py:810\u001b[39m, in \u001b[36mGraphRun.__anext__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    807\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._next_node, End):\n\u001b[32m    808\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m810\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.next(\u001b[38;5;28mself\u001b[39m._next_node)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pydantic_graph/graph.py:783\u001b[39m, in \u001b[36mGraphRun.next\u001b[39m\u001b[34m(self, node)\u001b[39m\n\u001b[32m    781\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.persistence.record_run(node_snapshot_id):\n\u001b[32m    782\u001b[39m         ctx = GraphRunContext(\u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.deps)\n\u001b[32m--> \u001b[39m\u001b[32m783\u001b[39m         \u001b[38;5;28mself\u001b[39m._next_node = \u001b[38;5;28;01mawait\u001b[39;00m node.run(ctx)\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._next_node, End):\n\u001b[32m    786\u001b[39m     \u001b[38;5;28mself\u001b[39m._snapshot_id = \u001b[38;5;28mself\u001b[39m._next_node.get_snapshot_id()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py:268\u001b[39m, in \u001b[36mModelRequestNode.run\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._did_stream:\n\u001b[32m    264\u001b[39m     \u001b[38;5;66;03m# `self._result` gets set when exiting the `stream` contextmanager, so hitting this\u001b[39;00m\n\u001b[32m    265\u001b[39m     \u001b[38;5;66;03m# means that the stream was started but not finished before `run()` was called\u001b[39;00m\n\u001b[32m    266\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.AgentRunError(\u001b[33m'\u001b[39m\u001b[33mYou must finish streaming before calling run()\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_request(ctx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py:322\u001b[39m, in \u001b[36mModelRequestNode._make_request\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m    320\u001b[39m model_settings, model_request_parameters = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._prepare_request(ctx)\n\u001b[32m    321\u001b[39m model_request_parameters = ctx.deps.model.customize_request_parameters(model_request_parameters)\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m model_response, request_usage = \u001b[38;5;28;01mawait\u001b[39;00m ctx.deps.model.request(\n\u001b[32m    323\u001b[39m     ctx.state.message_history, model_settings, model_request_parameters\n\u001b[32m    324\u001b[39m )\n\u001b[32m    325\u001b[39m ctx.state.usage.incr(_usage.Usage(), requests=\u001b[32m1\u001b[39m)\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._finish_handling(ctx, model_response, request_usage)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pydantic_ai/models/gemini.py:138\u001b[39m, in \u001b[36mGeminiModel.request\u001b[39m\u001b[34m(self, messages, model_settings, model_request_parameters)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m    132\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    133\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[ModelMessage],\n\u001b[32m    134\u001b[39m     model_settings: ModelSettings | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    135\u001b[39m     model_request_parameters: ModelRequestParameters,\n\u001b[32m    136\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ModelResponse, usage.Usage]:\n\u001b[32m    137\u001b[39m     check_allow_model_requests()\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_request(\n\u001b[32m    139\u001b[39m         messages, \u001b[38;5;28;01mFalse\u001b[39;00m, cast(GeminiModelSettings, model_settings \u001b[38;5;129;01mor\u001b[39;00m {}), model_request_parameters\n\u001b[32m    140\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m http_response:\n\u001b[32m    141\u001b[39m         data = \u001b[38;5;28;01mawait\u001b[39;00m http_response.aread()\n\u001b[32m    142\u001b[39m         response = _gemini_response_ta.validate_json(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/contextlib.py:210\u001b[39m, in \u001b[36m_AsyncGeneratorContextManager.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m anext(\u001b[38;5;28mself\u001b[39m.gen)\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pydantic_ai/models/gemini.py:235\u001b[39m, in \u001b[36mGeminiModel._make_request\u001b[39m\u001b[34m(self, messages, streamed, model_settings, model_request_parameters)\u001b[39m\n\u001b[32m    232\u001b[39m url = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33mstreamGenerateContent\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mstreamed\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mgenerateContent\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    234\u001b[39m request_json = _gemini_request_ta.dump_json(request_data, by_alias=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.stream(\n\u001b[32m    236\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    237\u001b[39m     url,\n\u001b[32m    238\u001b[39m     content=request_json,\n\u001b[32m    239\u001b[39m     headers=headers,\n\u001b[32m    240\u001b[39m     timeout=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m'\u001b[39m, USE_CLIENT_DEFAULT),\n\u001b[32m    241\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m r:\n\u001b[32m    242\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (status_code := r.status_code) != \u001b[32m200\u001b[39m:\n\u001b[32m    243\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m r.aread()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/contextlib.py:210\u001b[39m, in \u001b[36m_AsyncGeneratorContextManager.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m anext(\u001b[38;5;28mself\u001b[39m.gen)\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/httpx/_client.py:1583\u001b[39m, in \u001b[36mAsyncClient.stream\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m   1560\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1561\u001b[39m \u001b[33;03mAlternative to `httpx.request()` that streams the response body\u001b[39;00m\n\u001b[32m   1562\u001b[39m \u001b[33;03minstead of loading it into memory at once.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1568\u001b[39m \u001b[33;03m[0]: /quickstart#streaming-responses\u001b[39;00m\n\u001b[32m   1569\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1570\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m   1571\u001b[39m     method=method,\n\u001b[32m   1572\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1581\u001b[39m     extensions=extensions,\n\u001b[32m   1582\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1583\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.send(\n\u001b[32m   1584\u001b[39m     request=request,\n\u001b[32m   1585\u001b[39m     auth=auth,\n\u001b[32m   1586\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m   1587\u001b[39m     stream=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1588\u001b[39m )\n\u001b[32m   1589\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1590\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/httpx/_client.py:1629\u001b[39m, in \u001b[36mAsyncClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m   1625\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m   1627\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_auth(\n\u001b[32m   1630\u001b[39m     request,\n\u001b[32m   1631\u001b[39m     auth=auth,\n\u001b[32m   1632\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m   1633\u001b[39m     history=[],\n\u001b[32m   1634\u001b[39m )\n\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1636\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/httpx/_client.py:1654\u001b[39m, in \u001b[36mAsyncClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m   1652\u001b[39m auth_flow = auth.async_auth_flow(request)\n\u001b[32m   1653\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1654\u001b[39m     request = \u001b[38;5;28;01mawait\u001b[39;00m auth_flow.\u001b[34m__anext__\u001b[39m()\n\u001b[32m   1656\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m   1657\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_redirects(\n\u001b[32m   1658\u001b[39m             request,\n\u001b[32m   1659\u001b[39m             follow_redirects=follow_redirects,\n\u001b[32m   1660\u001b[39m             history=history,\n\u001b[32m   1661\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pydantic_ai/providers/google_vertex.py:132\u001b[39m, in \u001b[36m_VertexAIAuth.async_auth_flow\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34masync_auth_flow\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: httpx.Request) -> AsyncGenerator[httpx.Request, httpx.Response]:\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.credentials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m         \u001b[38;5;28mself\u001b[39m.credentials = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_credentials()\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.credentials.token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# type: ignore[reportUnknownMemberType]\u001b[39;00m\n\u001b[32m    134\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._refresh_token()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pydantic_ai/providers/google_vertex.py:157\u001b[39m, in \u001b[36m_VertexAIAuth._get_credentials\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    155\u001b[39m     creds_source = \u001b[33m'\u001b[39m\u001b[33mservice account info\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     creds, creds_project_id = \u001b[38;5;28;01mawait\u001b[39;00m _async_google_auth()\n\u001b[32m    158\u001b[39m     creds_source = \u001b[33m'\u001b[39m\u001b[33m`google.auth.default()`\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.project_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pydantic_ai/providers/google_vertex.py:174\u001b[39m, in \u001b[36m_async_google_auth\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_async_google_auth\u001b[39m() -> \u001b[38;5;28mtuple\u001b[39m[BaseCredentials, \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m anyio.to_thread.run_sync(google.auth.default, [\u001b[33m'\u001b[39m\u001b[33mhttps://www.googleapis.com/auth/cloud-platform\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/anyio/to_thread.py:56\u001b[39m, in \u001b[36mrun_sync\u001b[39m\u001b[34m(func, abandon_on_cancel, cancellable, limiter, *args)\u001b[39m\n\u001b[32m     48\u001b[39m     abandon_on_cancel = cancellable\n\u001b[32m     49\u001b[39m     warn(\n\u001b[32m     50\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe `cancellable=` keyword argument to `anyio.to_thread.run_sync` is \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     51\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdeprecated since AnyIO 4.1.0; use `abandon_on_cancel=` instead\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     52\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m     53\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m     54\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m get_async_backend().run_sync_in_worker_thread(\n\u001b[32m     57\u001b[39m     func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n\u001b[32m     58\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:2470\u001b[39m, in \u001b[36mAsyncIOBackend.run_sync_in_worker_thread\u001b[39m\u001b[34m(cls, func, args, abandon_on_cancel, limiter)\u001b[39m\n\u001b[32m   2467\u001b[39m     worker_scope = scope._parent_scope\n\u001b[32m   2469\u001b[39m worker.queue.put_nowait((context, func, args, future, worker_scope))\n\u001b[32m-> \u001b[39m\u001b[32m2470\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from __future__ import annotations as _annotations\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from pydantic import BaseModel, EmailStr\n",
    "\n",
    "from pydantic_ai import Agent, format_as_xml\n",
    "from pydantic_ai.messages import ModelMessage\n",
    "from pydantic_graph import BaseNode, End, Graph, GraphRunContext\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class User:\n",
    "    name: str\n",
    "    email: EmailStr\n",
    "    interests: list[str]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Email:\n",
    "    subject: str\n",
    "    body: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class State:\n",
    "    user: User\n",
    "    write_agent_messages: list[ModelMessage] = field(default_factory=list)\n",
    "\n",
    "\n",
    "email_writer_agent = Agent(\n",
    "    'google-vertex:gemini-1.5-pro',\n",
    "    output_type=Email,\n",
    "    system_prompt='Write a welcome email to our tech blog.',\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class WriteEmail(BaseNode[State]):\n",
    "    email_feedback: str | None = None\n",
    "\n",
    "    async def run(self, ctx: GraphRunContext[State]) -> Feedback:\n",
    "        if self.email_feedback:\n",
    "            prompt = (\n",
    "                f'Rewrite the email for the user:\\n'\n",
    "                f'{format_as_xml(ctx.state.user)}\\n'\n",
    "                f'Feedback: {self.email_feedback}'\n",
    "            )\n",
    "        else:\n",
    "            prompt = (\n",
    "                f'Write a welcome email for the user:\\n'\n",
    "                f'{format_as_xml(ctx.state.user)}'\n",
    "            )\n",
    "\n",
    "        result = await email_writer_agent.run(\n",
    "            prompt,\n",
    "            message_history=ctx.state.write_agent_messages,\n",
    "        )\n",
    "        ctx.state.write_agent_messages += result.new_messages()\n",
    "        return Feedback(result.output)\n",
    "\n",
    "\n",
    "class EmailRequiresWrite(BaseModel):\n",
    "    feedback: str\n",
    "\n",
    "\n",
    "class EmailOk(BaseModel):\n",
    "    pass\n",
    "\n",
    "\n",
    "feedback_agent = Agent[None, EmailRequiresWrite | EmailOk](\n",
    "    f'openai:{model_name}',\n",
    "    output_type=EmailRequiresWrite | EmailOk,  # type: ignore\n",
    "    system_prompt=(\n",
    "        'Review the email and provide feedback, email must reference the users specific interests.'\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Feedback(BaseNode[State, None, Email]):\n",
    "    email: Email\n",
    "\n",
    "    async def run(\n",
    "        self,\n",
    "        ctx: GraphRunContext[State],\n",
    "    ) -> WriteEmail | End[Email]:\n",
    "        prompt = format_as_xml({'user': ctx.state.user, 'email': self.email})\n",
    "        result = await feedback_agent.run(prompt)\n",
    "        if isinstance(result.output, EmailRequiresWrite):\n",
    "            return WriteEmail(email_feedback=result.output.feedback)\n",
    "        else:\n",
    "            return End(self.email)\n",
    "\n",
    "\n",
    "async def main():\n",
    "    user = User(\n",
    "        name='John Doe',\n",
    "        email='john.joe@example.com',\n",
    "        interests=['Haskel', 'Lisp', 'Fortran'],\n",
    "    )\n",
    "    state = State(user)\n",
    "    feedback_graph = Graph(nodes=(WriteEmail, Feedback))\n",
    "    result = await feedback_graph.run(WriteEmail(), state=state)\n",
    "    print(result.output)\n",
    "    \"\"\"\n",
    "    Email(\n",
    "        subject='Welcome to our tech blog!',\n",
    "        body='Hello John, Welcome to our tech blog! ...',\n",
    "    )\n",
    "    \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "db9577c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "title: vending_machine_graph\n",
      "---\n",
      "stateDiagram-v2\n",
      "  [*] --> InsertCoin\n",
      "  InsertCoin --> CoinsInserted\n",
      "  CoinsInserted --> SelectProduct\n",
      "  CoinsInserted --> Purchase\n",
      "  SelectProduct --> Purchase\n",
      "  Purchase --> InsertCoin\n",
      "  Purchase --> SelectProduct\n",
      "  Purchase --> [*]\n"
     ]
    }
   ],
   "source": [
    "print(vending_machine_graph.mermaid_code(start_node=InsertCoin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9569935b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n"
     ]
    }
   ],
   "source": [
    "!pip install nest_asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56f2712c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store store_x received order from customer_1 for 3xP123\n",
      "Store store_x fulfilled order for customer_1: 3xP123\n",
      "Store store_x received order from customer_2 for 3xP123\n",
      "Store store_x fulfilled order for customer_2: 3xP123\n",
      "Customer customer_1 successfully ordered 3xP123 from store_x.\n",
      "Customer customer_2 successfully ordered 3xP123 from store_x.\n",
      "Store store_x received order from customer_1 for 3xP123\n",
      "Store store_x fulfilled order for customer_1: 3xP123\n",
      "Customer customer_1 successfully ordered 3xP123 from store_x.\n",
      "Store store_x received order from customer_2 for 2xP123\n",
      "Store store_x fulfilled order for customer_2: 2xP123\n",
      "Customer customer_2 successfully ordered 2xP123 from store_x.\n",
      "[store_x] Triggering restock for P123.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'coroutine' object has no attribute 'gi_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/simpy/events.py:431\u001b[39m, in \u001b[36mProcess._resume\u001b[39m\u001b[34m(self, event)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    430\u001b[39m     \u001b[38;5;66;03m# Be optimistic and blindly access the callbacks attribute.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mevent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    432\u001b[39m         \u001b[38;5;66;03m# The event has not yet been triggered. Register callback\u001b[39;00m\n\u001b[32m    433\u001b[39m         \u001b[38;5;66;03m# to resume the process if that happens.\u001b[39;00m\n\u001b[32m    434\u001b[39m         event.callbacks.append(\u001b[38;5;28mself\u001b[39m._resume)\n",
      "\u001b[31mAttributeError\u001b[39m: '_asyncio.Future' object has no attribute 'callbacks'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 474\u001b[39m\n\u001b[32m    471\u001b[39m ecosystem.add_customer(customer_2)\n\u001b[32m    473\u001b[39m \u001b[38;5;66;03m# Simulate the ecosystem\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m \u001b[43mecosystem\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mduration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mduration\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 373\u001b[39m, in \u001b[36mRetailEcosystem.simulate\u001b[39m\u001b[34m(self, duration)\u001b[39m\n\u001b[32m    370\u001b[39m     \u001b[38;5;28mself\u001b[39m.env.process(\u001b[38;5;28mself\u001b[39m.supplier_behavior(supplier))\n\u001b[32m    372\u001b[39m \u001b[38;5;66;03m# Start simulation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43muntil\u001b[49m\u001b[43m=\u001b[49m\u001b[43mduration\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/simpy/core.py:246\u001b[39m, in \u001b[36mEnvironment.run\u001b[39m\u001b[34m(self, until)\u001b[39m\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m StopSimulation \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc.args[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# == until.value\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/simpy/core.py:196\u001b[39m, in \u001b[36mEnvironment.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    194\u001b[39m callbacks, event.callbacks = event.callbacks, \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m event._ok \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(event, \u001b[33m'\u001b[39m\u001b[33m_defused\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    199\u001b[39m     \u001b[38;5;66;03m# The event has failed and has not been defused. Crash the\u001b[39;00m\n\u001b[32m    200\u001b[39m     \u001b[38;5;66;03m# environment.\u001b[39;00m\n\u001b[32m    201\u001b[39m     \u001b[38;5;66;03m# Create a copy of the failure exception with a new traceback.\u001b[39;00m\n\u001b[32m    202\u001b[39m     exc = \u001b[38;5;28mtype\u001b[39m(event._value)(*event._value.args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/simpy/events.py:443\u001b[39m, in \u001b[36mProcess._resume\u001b[39m\u001b[34m(self, event)\u001b[39m\n\u001b[32m    440\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    442\u001b[39m         msg = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mInvalid yield value \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m         descr = _describe_frame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgi_frame\u001b[49m)\n\u001b[32m    444\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdescr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    446\u001b[39m \u001b[38;5;28mself\u001b[39m._target = event\n",
      "\u001b[31mAttributeError\u001b[39m: 'coroutine' object has no attribute 'gi_frame'"
     ]
    }
   ],
   "source": [
    "import simpy\n",
    "import asyncio\n",
    "import random\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any, Dict, Callable, List\n",
    "\n",
    "class Artifact(ABC):\n",
    "    \"\"\"A data object that agents interact with.\"\"\"\n",
    "    pass\n",
    "\n",
    "class Context(ABC):\n",
    "    \"\"\"Environmental factors influencing actions.\"\"\"\n",
    "    data: Dict[str, Any]\n",
    "\n",
    "    def __init__(self, **data):\n",
    "        self.data = data\n",
    "\n",
    "class Profile(ABC):\n",
    "    \"\"\"Behavioral profile encapsulating decision parameters for an Agent.\"\"\"\n",
    "    @abstractmethod\n",
    "    def parameters(self) -> Dict[str, Any]:\n",
    "        pass\n",
    "\n",
    "class Policy(ABC):\n",
    "    \"\"\"Rule or strategy that conditions agent behavior.\"\"\"\n",
    "    @abstractmethod\n",
    "    def evaluate(self, agent: 'Agent', artifact: Artifact, context: Context) -> bool:\n",
    "        pass\n",
    "\n",
    "class Event:\n",
    "    \"\"\"An occurrence triggered by an action.\"\"\"\n",
    "    def __init__(self, name: str, payload: Dict[str, Any] = None):\n",
    "        self.name = name\n",
    "        self.payload = payload or {}\n",
    "\n",
    "# Event registry and decorator\n",
    "_EVENT_HANDLERS: Dict[str, List[Callable[[Event], None]]] = {}\n",
    "\n",
    "def on_event(event_name: str):\n",
    "    \"\"\"Decorator to subscribe a handler to an event.\"\"\"\n",
    "    def decorator(fn: Callable[[Event], None]):\n",
    "        _EVENT_HANDLERS.setdefault(event_name, []).append(fn)\n",
    "        return fn\n",
    "    return decorator\n",
    "\n",
    "def emit(event: Event):\n",
    "    \"\"\"Emit an event to all subscribers.\"\"\"\n",
    "    for handler in _EVENT_HANDLERS.get(event.name, []):\n",
    "        handler(event)\n",
    "\n",
    "from typing import Dict, Any, Callable\n",
    "from abc import ABC\n",
    "\n",
    "class Agent(ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: simpy.Environment,\n",
    "        id: str,\n",
    "        profile: Profile = None,\n",
    "        policy: Policy = None,\n",
    "        actions: Dict[str, Callable] = None\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.id = id\n",
    "        self.profile = profile\n",
    "        self.policy = policy\n",
    "        self.actions = actions or {}\n",
    "        self._register_actions()\n",
    "\n",
    "    def _register_actions(self):\n",
    "        for name in dir(self):\n",
    "            if callable(getattr(self, name)) and not name.startswith(\"_\"):\n",
    "                self.actions.setdefault(name, getattr(self, name))\n",
    "\n",
    "    async def perform(\n",
    "        self,\n",
    "        action: str,\n",
    "        artifact: Artifact,\n",
    "        context: Dict[str, Context],\n",
    "        **kwargs\n",
    "    ) -> Any:\n",
    "        if action not in self.actions:\n",
    "            raise ValueError(f\"Action '{action}' not found for agent '{self.id}'\")\n",
    "\n",
    "        method = self.actions[action]\n",
    "\n",
    "        result = await self._consume_async_generator(\n",
    "            method(artifact, context, **kwargs)\n",
    "        )\n",
    "\n",
    "        ev = Event(f\"{self.__class__.__name__}.{action}\", {\n",
    "            'agent_id': self.id,\n",
    "            'artifact': artifact,\n",
    "            'context': {k: v.dict() if hasattr(v, 'dict') else str(v) for k, v in context.items()},\n",
    "            'result': result,\n",
    "        })\n",
    "        emit(ev)\n",
    "        return result\n",
    "\n",
    "    async def _consume_async_generator(self, async_gen) -> Any:\n",
    "        \"\"\"Consume the async generator and return the final value.\"\"\"\n",
    "        async for item in async_gen:\n",
    "            # Return the last yielded value\n",
    "            pass\n",
    "        return item\n",
    "\n",
    "# --- Example Implementations --- #\n",
    "\n",
    "# Artifact subclass\n",
    "class Product(Artifact):\n",
    "    def __init__(self, sku: str, price: float):\n",
    "        self.sku = sku\n",
    "        self.price = price\n",
    "\n",
    "class Inventory(Artifact):\n",
    "    \"\"\"Represents the inventory in a store.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.items = {}  # Mapping from product SKU to quantity\n",
    "\n",
    "    def add_product(self, product: Product, quantity: int):\n",
    "        \"\"\"Add products to the inventory.\"\"\"\n",
    "        if product.sku in self.items:\n",
    "            self.items[product.sku] += quantity\n",
    "        else:\n",
    "            self.items[product.sku] = quantity\n",
    "\n",
    "    def remove_product(self, product: Product, quantity: int):\n",
    "        \"\"\"Remove products from inventory.\"\"\"\n",
    "        if product.sku in self.items and self.items[product.sku] >= quantity:\n",
    "            self.items[product.sku] -= quantity\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def check_stock(self, product: Product) -> int:\n",
    "        \"\"\"Check the stock of a specific product.\"\"\"\n",
    "        return self.items.get(product.sku, 0)\n",
    "\n",
    "\n",
    "# Context subclass\n",
    "class MarketContext(Context):\n",
    "    pass\n",
    "\n",
    "class LeadTime(Context):\n",
    "    \"\"\"Represents the lead time for inventory restocking and product arrival.\"\"\"\n",
    "    \n",
    "    def __init__(self, restocking_lead_time: int, delivery_lead_time: int):\n",
    "        super().__init__(\n",
    "            restocking_lead_time=restocking_lead_time, \n",
    "            delivery_lead_time=delivery_lead_time\n",
    "        )\n",
    "        self.restocking_lead_time = restocking_lead_time\n",
    "        self.delivery_lead_time = delivery_lead_time\n",
    "\n",
    "# Profile subclass\n",
    "class CustomerProfile(Profile):\n",
    "    def __init__(self, price_sensitivity: float):\n",
    "        self.price_sensitivity = price_sensitivity\n",
    "    def parameters(self) -> Dict[str, Any]:\n",
    "        return {'price_sensitivity': self.price_sensitivity}\n",
    "\n",
    "# Policy subclass\n",
    "class PurchasePolicy(Policy):\n",
    "    def __init__(self, threshold: float):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def evaluate(self, agent: Agent, artifact: Product, context: MarketContext) -> bool:\n",
    "        sensitivity = agent.profile.parameters()['price_sensitivity']\n",
    "        return artifact.price < self.threshold * sensitivity\n",
    "\n",
    "# Agent subclass\n",
    "class Customer(Agent):\n",
    "    def __init__(self, env: simpy.Environment, id: str, profile: CustomerProfile, policy: PurchasePolicy):\n",
    "        super().__init__(env, id, profile, policy)\n",
    "\n",
    "    def define_actions(self) -> Dict[str, Callable]:\n",
    "        return {\n",
    "            'place_order': self.place_order,\n",
    "            'add_to_cart': self.add_to_cart\n",
    "        }\n",
    "\n",
    "    async def place_order(self, store: \"Store\", sku: str, quantity: int, context: dict):\n",
    "        return await store.process_order(self, sku, quantity, context)\n",
    "\n",
    "    async def add_to_cart(self, product: Product, context: Dict[str, Context], quantity: int = 1):\n",
    "        yield self.env.timeout(0.5)\n",
    "        print(f\"Customer {self.id} added {quantity}x {product.sku} to cart.\")\n",
    "        yield True\n",
    "\n",
    "class InventoryManager(Agent):\n",
    "    def __init__(\n",
    "        self, env: simpy.Environment, id: str, \n",
    "        profile: Profile, policy: Policy, \n",
    "        inventory: Inventory\n",
    "    ):\n",
    "        super().__init__(env, id, profile, policy)\n",
    "        self.inventory = inventory\n",
    "\n",
    "    async def manage_inventory(\n",
    "        self, \n",
    "        product: Product, context: Dict[str, Context], \n",
    "        quantity: int, action: str\n",
    "    ) -> bool:\n",
    "        await asyncio.sleep(0.1)\n",
    "        if action == \"add\":\n",
    "            self.inventory.add_product(product, quantity)\n",
    "            return True\n",
    "        elif action == \"remove\":\n",
    "            self.inventory.remove_product(product, quantity)\n",
    "            return True\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown action '{action}'\")\n",
    "\n",
    "class Salesman(Agent):\n",
    "    def __init__(\n",
    "        self, env: simpy.Environment, id: str, \n",
    "        profile: Profile, policy: Policy, \n",
    "        inventory: Inventory\n",
    "    ):\n",
    "        super().__init__(env, id, profile, policy)\n",
    "        self.inventory = inventory\n",
    "\n",
    "    async def sell(\n",
    "        self, product: Product, context: MarketContext, quantity: int\n",
    "    ) -> bool:\n",
    "        await asyncio.sleep(1)\n",
    "        if self.inventory.check_stock(product) >= quantity:\n",
    "            self.inventory.remove_product(product, quantity)\n",
    "            print(f\"Salesman {self.id} sold {quantity}x {product.sku}.\")\n",
    "            return True\n",
    "        print(f\"Salesman {self.id} could not sell {quantity}x {product.sku} due to stock shortage.\")\n",
    "        return False\n",
    "\n",
    "class Supplier(Agent):\n",
    "    def __init__(\n",
    "        self, env: simpy.Environment, \n",
    "        id: str, inventory: Inventory, \n",
    "        supply_time: float = 1.0\n",
    "    ):\n",
    "        super().__init__(env, id)\n",
    "        self.inventory = inventory  # Supplier's own inventory\n",
    "        self.supply_time = supply_time\n",
    "\n",
    "    async def supply_product(self, product: Product, quantity: int, inventory: Inventory) -> bool:\n",
    "        \"\"\"Supply a specific store with a product.\"\"\"\n",
    "        print(f\"Supplier {self.id} is supplying {quantity}x {product.sku} to {store.id}.\")\n",
    "        await asyncio.sleep(1)  # Simulate supply time\n",
    "        \n",
    "        # Check if supplier has enough product\n",
    "        if self.inventory.check_stock(product) >= quantity:\n",
    "            # Decrease the supplier's stock\n",
    "            self.inventory.remove_product(product, quantity)\n",
    "\n",
    "            # Supply time is simulated\n",
    "            await asyncio.sleep(self.supply_time)\n",
    "\n",
    "            # Restock the store's inventory\n",
    "            inventory.add_product(product, quantity)\n",
    "            print(f\"Supplier {self.id} supplied {quantity}x{product.sku} to {store.id}.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Supplier {self.id} does not have enough {product.sku}.\")\n",
    "            return False\n",
    "\n",
    "class Store(Agent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: simpy.Environment, id: str, \n",
    "        inventory: Inventory,\n",
    "        suppliers: List[Supplier],\n",
    "        num_salesmen: int = 5,\n",
    "        restock_threshold: int = 10\n",
    "    ):\n",
    "        super().__init__(env, id)\n",
    "        self.inventory = inventory\n",
    "        self.suppliers = suppliers\n",
    "        self.restock_threshold = restock_threshold\n",
    "\n",
    "        # Staff members\n",
    "        self.inventory_manager = InventoryManager(\n",
    "            env, f\"{id}_inv_mgr\", \n",
    "            profile=None, policy=None, \n",
    "            inventory=inventory\n",
    "        )\n",
    "        self.salesmen = [\n",
    "            Salesman(\n",
    "                env, f\"{id}_salesman_{i}\", \n",
    "                profile=None, policy=None, \n",
    "                inventory=inventory\n",
    "            )\n",
    "            for i in range(num_salesmen)\n",
    "        ]\n",
    "\n",
    "    def process_order(self, customer, sku, quantity, context):\n",
    "        def _inner():\n",
    "            print(f\"Store {self.id} received order from {customer.id} for {quantity}x{sku}\")\n",
    "            available_qty = self.inventory.check_stock(Product(sku, 0))\n",
    "            if available_qty >= quantity:\n",
    "                self.inventory.remove_product(Product(sku, 0), quantity)\n",
    "                print(f\"Store {self.id} fulfilled order for {customer.id}: {quantity}x{sku}\")\n",
    "                result = True\n",
    "            else:\n",
    "                print(f\"Store {self.id} cannot fulfill order for {customer.id}: {quantity}x{sku}\")\n",
    "                result = False\n",
    "            yield self.env.timeout(0)  # Optional delay\n",
    "            return result\n",
    "        return _inner()  # ✅ Return the generator, not the result\n",
    "\n",
    "\n",
    "    async def restock_inventory(\n",
    "        self, product: Product, context: Dict[str, Context], quantity: int\n",
    "    ) -> bool:\n",
    "        current_stock = self.inventory.check_stock(product)\n",
    "        if current_stock >= self.restock_threshold:\n",
    "            print(f\"[{self.id}] No need to restock {product.sku} (stock: {current_stock}).\")\n",
    "            return False\n",
    "        \n",
    "        # Delegate to inventory manager who may coordinate with suppliers\n",
    "        print(f\"[{self.id}] Triggering restock for {product.sku}.\")\n",
    "        return await self.inventory_manager.manage_inventory(product, context, quantity, action=\"add\")\n",
    "\n",
    "    async def handle_sale(self, product: Product, context: Dict[str, Context], quantity: int) -> bool:\n",
    "        # Random or round-robin strategy could be used here\n",
    "        salesman = random.choice(self.salesmen)\n",
    "        success = await salesman.sell(product, context, quantity)\n",
    "        if success:\n",
    "            print(f\"[{self.id}] Sale successful: {quantity}x{product.sku}.\")\n",
    "        else:\n",
    "            print(f\"[{self.id}] Sale failed: Insufficient stock for {product.sku}.\")\n",
    "        return success\n",
    "\n",
    "class RetailEcosystem:\n",
    "    def __init__(self, env: simpy.Environment):\n",
    "        self.env = env\n",
    "        self.stores = []\n",
    "        self.suppliers = []\n",
    "        self.customers = []\n",
    "\n",
    "    def add_store(self, store: Store):\n",
    "        self.stores.append(store)\n",
    "\n",
    "    def add_supplier(self, supplier: Supplier):\n",
    "        self.suppliers.append(supplier)\n",
    "\n",
    "    def add_customer(self, customer: Customer):\n",
    "        self.customers.append(customer)\n",
    "\n",
    "    def get_global_catalog(self):\n",
    "        result = []\n",
    "        for store in self.stores:\n",
    "            for sku, quantity in store.inventory.items.items():\n",
    "                if quantity > 0:\n",
    "                    result.append({\n",
    "                        \"store_id\": store.id,\n",
    "                        \"product\": sku,\n",
    "                        \"available\": True,\n",
    "                    })\n",
    "        return result\n",
    "\n",
    "    def simulate(self, duration: int = 100):\n",
    "        # Register all agent behaviors as processes\n",
    "        for store in self.stores:\n",
    "            self.env.process(self.store_behavior(store))\n",
    "\n",
    "        for customer in self.customers:\n",
    "            self.env.process(self.customer_behavior(customer))\n",
    "\n",
    "        for supplier in self.suppliers:\n",
    "            self.env.process(self.supplier_behavior(supplier))\n",
    "\n",
    "        # Start simulation\n",
    "        self.env.run(until=duration)\n",
    "\n",
    "    def store_behavior(self, store: Store):\n",
    "        while True:\n",
    "            for product_sku, quantity in store.inventory.items.items():\n",
    "                if quantity < store.restock_threshold:\n",
    "                    product = Product(sku=product_sku, price=10.0)\n",
    "                    context = {\"time\": {\"lead_time\": LeadTime(3, 2)}}\n",
    "                    yield self.env.process(store.restock_inventory(product, context, quantity=50))\n",
    "            yield self.env.timeout(10)  # Check every X time units\n",
    "\n",
    "    def customer_behavior(self, customer: Customer):\n",
    "        while True:\n",
    "            catalog = self.get_global_catalog()\n",
    "            if not catalog:\n",
    "                print(f\"No available products for {customer.id}\")\n",
    "                yield self.env.timeout(5)\n",
    "                continue\n",
    "\n",
    "            item = random.choice(catalog)\n",
    "            store_id = item[\"store_id\"]\n",
    "            sku = item[\"product\"]\n",
    "            store = next((s for s in self.stores if s.id == store_id), None)\n",
    "            if not store:\n",
    "                yield self.env.timeout(1)\n",
    "                continue\n",
    "\n",
    "            quantity = random.randint(1, 5)\n",
    "            context = {\"market\": {\"lead_time\": LeadTime(5, 2)}}\n",
    "\n",
    "            # Now simulate customer placing order\n",
    "            success = yield self.env.process(store.process_order(customer, sku, quantity, context))\n",
    "\n",
    "            if success:\n",
    "                print(f\"Customer {customer.id} successfully ordered {quantity}x{sku} from {store_id}.\")\n",
    "            else:\n",
    "                print(f\"Customer {customer.id} failed to order {sku} from {store_id}.\")\n",
    "\n",
    "            yield self.env.timeout(random.randint(10, 20))  # Wait before next order\n",
    "\n",
    "\n",
    "    def supplier_behavior(self, supplier: Supplier):\n",
    "        while True:\n",
    "            # Placeholder for supplier behavior (e.g., batching, responding to orders)\n",
    "            yield self.env.timeout(1)\n",
    "\n",
    "# Create simulation environment\n",
    "env = simpy.Environment()\n",
    "\n",
    "# Create products\n",
    "product_1 = Product(sku=\"P123\", price=5.0)\n",
    "product_2 = Product(sku=\"P456\", price=30.0)\n",
    "\n",
    "# Create inventory, stores, suppliers, and customers\n",
    "supplier_1_inventory = Inventory()\n",
    "supplier_1_inventory.add_product(product_1, 100)\n",
    "supplier_1 = Supplier(env, id=\"supplier_1\", inventory=supplier_1_inventory)\n",
    "\n",
    "# Create a product and add it to the supplier's inventory\n",
    "supplier_2_inventory = Inventory()\n",
    "supplier_2 = Supplier(env, id=\"supplier_2\", inventory=supplier_2_inventory)\n",
    "\n",
    "supplier_2_inventory.add_product(product_1, 50)\n",
    "supplier_2_inventory.add_product(product_2, 200)\n",
    "\n",
    "# Create a store\n",
    "store_inventory = Inventory()\n",
    "store_inventory.add_product(product_1, 20)\n",
    "\n",
    "store = Store(\n",
    "    env, id=\"store_x\", \n",
    "    inventory=store_inventory, \n",
    "    suppliers=[supplier_1, supplier_2], \n",
    "    num_salesmen=3, \n",
    "    restock_threshold=10\n",
    ")\n",
    "\n",
    "# Create and add to Retail Ecosystem\n",
    "duration = 30\n",
    "ecosystem = RetailEcosystem(env)\n",
    "ecosystem.add_store(store)\n",
    "\n",
    "ecosystem.add_supplier(supplier_1)\n",
    "ecosystem.add_supplier(supplier_2)\n",
    "\n",
    "# Add a customer for testing\n",
    "customer_1 = Customer(\n",
    "    env=env, id=\"customer_1\", \n",
    "    profile=CustomerProfile(price_sensitivity=0.8), \n",
    "    policy=PurchasePolicy(10.0)\n",
    ")\n",
    "customer_2 = Customer(\n",
    "    env=env, id=\"customer_2\", \n",
    "    profile=CustomerProfile(price_sensitivity=1), \n",
    "    policy=PurchasePolicy(30.0)\n",
    ")\n",
    "\n",
    "ecosystem.add_customer(customer_1)\n",
    "ecosystem.add_customer(customer_2)\n",
    "\n",
    "# Simulate the ecosystem\n",
    "ecosystem.simulate(duration=duration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d84f66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
